{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AUCO ResNet Official Tensorflow-Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincenzodentamaro/aucoresnet/blob/main/AUCO_ResNet_Official_Tensorflow_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdbGOQcT0dz",
        "outputId": "57c98d24-f352-4318-aadd-19fd8d29f695"
      },
      "source": [
        "!pip install kapre==0.3.4\n",
        "#pip install git https://github.com/user/repo.git@branch\n",
        "!pip install --upgrade tf_siren\n",
        "!pip install --upgrade googledrivedownloader"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kapre==0.3.4 in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from kapre==0.3.4) (1.21.5)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from kapre==0.3.4) (2.8.0)\n",
            "Requirement already satisfied: librosa>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from kapre==0.3.4) (0.8.1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (1.1.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (1.6.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (0.51.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (21.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (1.4.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->kapre==0.3.4) (4.4.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.7.2->kapre==0.3.4) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.7.2->kapre==0.3.4) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa>=0.7.2->kapre==0.3.4) (3.0.7)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.7.2->kapre==0.3.4) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.7.2->kapre==0.3.4) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre==0.3.4) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre==0.3.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre==0.3.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.7.2->kapre==0.3.4) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa>=0.7.2->kapre==0.3.4) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.7.2->kapre==0.3.4) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.7.2->kapre==0.3.4) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.7.2->kapre==0.3.4) (2.21)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (0.24.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (13.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (1.44.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (0.5.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (1.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (1.13.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->kapre==0.3.4) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->kapre==0.3.4) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0->kapre==0.3.4) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->kapre==0.3.4) (3.2.0)\n",
            "Requirement already satisfied: tf_siren in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tf_siren) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.44.1 in /usr/local/lib/python3.7/dist-packages (from tf_siren) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from tf_siren) (1.21.5)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (0.24.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (13.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.44.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (0.5.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.15.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (1.13.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tf_siren) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->tf_siren) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->tf_siren) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tf_siren) (3.2.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMczZvA5Q29B",
        "outputId": "46e17cc1-be25-4d9e-c038-fdde225247c3"
      },
      "source": [
        "print('Running in 1-thread CPU mode for fully reproducible results training a CNN and generating numpy randomness.  This mode may be slow...')\n",
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 1\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "seed_value += 1\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "seed_value += 1\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "seed_value += 1\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed_value)\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in 1-thread CPU mode for fully reproducible results training a CNN and generating numpy randomness.  This mode may be slow...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a toy dataset extracted from Virufy, thus accuracies are unreliable and not untruthful"
      ],
      "metadata": {
        "id": "J2uCGQYdu0p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vincenzodentamaro/aucoresnet.git\n",
        "!unzip aucoresnet/virufy_pickle.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX-X79vOgXYY",
        "outputId": "dad84d44-7c95-47cb-d273-f88c671d09c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'aucoresnet' already exists and is not an empty directory.\n",
            "Archive:  aucoresnet/virufy_pickle.zip\n",
            "  inflating: Virufy_cough_noSplit_POS.p  \n",
            "  inflating: Virufy_cough_noSplit_NEG.p  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7xXKYzHG3kH"
      },
      "source": [
        "import pickle\n",
        "cam_task1_pos = pickle.load( open( \"Virufy_cough_noSplit_POS.p\", \"rb\" ) )\n",
        "cam_task1_neg = pickle.load( open( \"Virufy_cough_noSplit_NEG.p\", \"rb\" ) )\n",
        "\n",
        "SAMPLING_RATE = 22050\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tmMQJyq4dEl6",
        "outputId": "8a027159-a522-4192-ffdb-1043f49b4fcf"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "patientids = []\n",
        "for value in cam_task1_pos:\n",
        "    patientid = value['id']\n",
        "    for audio in value['audio']:\n",
        "        X.append(audio)\n",
        "        y.append(1.0)\n",
        "        patientids.append(patientid)\n",
        "        \n",
        "for value in cam_task1_neg:\n",
        "    patientid = value['id']\n",
        "    for audio in value['audio']:\n",
        "        X.append(audio)\n",
        "        y.append(0.0)\n",
        "        patientids.append(patientid)\n",
        "'''\n",
        "\n",
        "for value in cos_breath_pos:\n",
        "    patientid = value['id']\n",
        "    for audio in value['audio']:\n",
        "        X.append(audio)\n",
        "        y.append(1.0)\n",
        "        patientids.append(patientid)\n",
        "        \n",
        "for value in cos_breath_neg:\n",
        "    patientid = value['id']\n",
        "    for audio in value['audio']:\n",
        "        X.append(audio)\n",
        "        y.append(0.0)\n",
        "        patientids.append(patientid)\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nfor value in cos_breath_pos:\\n    patientid = value['id']\\n    for audio in value['audio']:\\n        X.append(audio)\\n        y.append(1.0)\\n        patientids.append(patientid)\\n        \\nfor value in cos_breath_neg:\\n    patientid = value['id']\\n    for audio in value['audio']:\\n        X.append(audio)\\n        y.append(0.0)\\n        patientids.append(patientid)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "QIBsgeWg1R3x",
        "outputId": "13409748-3e9b-455a-a226-aa43b5edc8bd"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lunghezze = []\n",
        "for r in X:\n",
        "  lunghezze.append(len(r))\n",
        "\n",
        "print('Mean secs')\n",
        "print(np.mean(lunghezze)/SAMPLING_RATE)\n",
        "print('Max secs')\n",
        "print(np.max(lunghezze)/SAMPLING_RATE)\n",
        "_ = plt.hist(np.asarray(lunghezze)/SAMPLING_RATE, bins=50)  # arguments are passed to np.histogram\n",
        "plt.title(\"Histogram with 'auto' bins\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean secs\n",
            "13.732426303854876\n",
            "Max secs\n",
            "19.340045351473922\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXcklEQVR4nO3df7RdZX3n8ffH8MMqlB/NxQpJCNo4FVRA7wQsOkBVCFWJs4ZZJkUNLTSrKHZKO+0A7QADayxFl1YHFFPNQqsCisCkYxDSWqAU0+ZCEQyCxABNAppI+CkIDXzmj/NENjfn3HNu7rk/ePJ5rXXW3ft5nr3Pd+/A5+yzzz5nyzYREVGvl012ARERMb4S9BERlUvQR0RULkEfEVG5BH1EROUS9BERlUvQR0eSVks6arLrmAoknSXpCyP0nyTp5omsqd+6bYOkayUtmsiaoj8S9DsoSfdLeuewthf9j277INs3dFnPbEmWtNM4lTol2P6Y7VOgP9tc9v/ssdYl6VxJXxnl+HO357lsH2f7S9uzbEyuBH1MabW/gERMhAR9dNQ86pc0V9KQpMcl/UTSJ8uwm8rfRyU9Kemtkl4m6c8lPSBpo6QvS9qjsd4Plb6HJf3PYc9zrqQrJX1F0uPASeW5vyvpUUkPSbpI0i6N9VnShyXdK+kJSedLeq2kW0q9X2+OH7aND0h6S5k+sazroDJ/sqRrGnVtPXLeZpsb6/uEpEck3SfpuB7387sl/WupdV3ziFvSUZLWt/t3kTQPOAt4f6nje6V/X0nLJG2WtEbS7/VSxwur10WSHpN0t6R3NDpukLT1Xc1Jkm7utL2lf23597hP0omjqCH6LEEfvfo08Gnbvwy8Fvh6af9P5e+etnez/V3gpPI4GngNsBtwEYCkA4HPAicCrwb2APYb9lzzgSuBPYGvAs8BpwPTgbcC7wA+PGyZY4G3AIcDfwosAT4AzATeACzssF03AkeV6SOBtY1tOrL0D9dumwEOA+4pdV4IfFGS2j2p7dm27y+zPwM+VLb33cCpkt7Xod7mOr4NfAy4otRxcOm6HFgP7AucAHxM0m+WZc61fe4Iqz0M+FHZhnOAqyTtPcLYbbZX0iuBzwDH2d4d+A3g9m7bE+MnQb9ju6YcJT8q6VFaAdzJvwO/Jmm67Sdtrxxh7InAJ22vtf0kcCawoJyGOQH4W9s3234WOBsY/oNL37V9je3nbT9t+1bbK21vKeH4eVoh3HSh7cdtrwa+D1xfnv8x4Frg0A613thY19uBv2jMdwr6Th6w/de2nwO+ROuF7FXdFrJ9g+07y/beAVzWZvt6ImkmcATwP2z/3PbtwBdovZD0YiPwV7b/3fYVtIL83R3GjrS9zwNvkPRLth8q/y4xSRL0O7b32d5z64Ntj5KbTgZeB9wtaZWk94wwdl/ggcb8A8BOtEJgX2Dd1g7bTwEPD1t+XXNG0usk/T9JPy6ncz5G6yiy6SeN6afbzO/WodYbgbdLejUwjdY7lSPKB6V7MLoj0R9vnSjbxQjP+wuSDpP0D5I2SXoM+H223b5e7Qtstv1Eo+0Btn3X1MkGv/iXDh8o62yn7fba/hnwflrb8ZCkb0n69R6fP8ZBgj56Yvte2wuBfYC/BK4sb9Hb/fzpg8D+jflZwBZa4fsQMGNrh6RfAn5l+NMNm/8ccDcwp5w6Ogtoe0pktGyvAZ4CPgrcZPtxWgG2GLjZ9vPtFuvHczd8DVgGzLS9B3AJL2zfz4BXbB0oaRowMEItDwJ7S9q90TYL2NBjLfsNO900q6xzVGxfZ/tdtI7y7wb+erTriP5J0EdPJH1A0kAJvkdL8/PApvL3NY3hlwGnSzpA0m68cB55C61z7++V9BvlA9Jz6R7auwOPA0+WI8NT+7VdxY3AabxwmuaGYfPDtdvmsdid1lH4zyXNBX670fdD4OXlA9udgT8Hdm30/wSYLellALbXAbcAfyHp5ZLeROvdWK+XYO4D/IGknSX9V+D1wPLRbIykV0maXw4EngGepLW/YpIk6KNX84DVkp6k9cHsgnL+/CngfwP/VM71Hw4sBf6G1tUp9wE/p3XETDlX+1FaHxg+RCsENtIKhE7+O63we4LWkeEVfd62G2mF7U0d5l+kwzaPxYeB8yQ9Qeszi60fdFM+Y/gwrfPsG2gd4TevwvlG+fuwpNvK9EJgNq0j8auBc2z/XY+1/DMwB/gprW08wfbwU2vdvAz4o/L8m2l93tDvF+cYBeXGIzGZyhH/o7ROy9w32fVE1ChH9DHhJL1X0ivKW/tPAHcC909uVRH1StDHZJhP6239g7ROEyxw3lpGjJucuomIqFyO6CMiKjclfzBq+vTpnj179mSXERHxknHrrbf+1PZAu74pGfSzZ89maGhossuIiHjJkPRAp76cuomIqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicl2DXtLMclOEuyStlvTf2oyRpM+U+1PeIenNjb5Fat3L815Ji/q9ARERMbJerqPfAvyx7dvKzQxulbTC9l2NMcfR+s2SObTuI/k54LByr8lzgEFaN0i4VdIy24/0dSsiIqKjrkf05X6Pt5XpJ4Af0P5mzl92y0pgz3JrtmOBFbY3l3BfQet3zSMiYoKM6pux5T6ah9K6OUHTfrz4Pp/rS1un9nbrXkzr9m3MmjVrNGXt8Gaf8a227fdf0Omezi99nba5k5r3RUQ3PX8YW24Q8U3gD8t9NfvK9hLbg7YHBwba/lxDRERsh56Cvtyr8pvAV21f1WbIBmBmY35GaevUHhERE6SXq24EfBH4ge1Pdhi2DPhQufrmcOAx2w8B1wHHSNpL0l7AMaUtIiImSC/n6I8APgjcKen20nYWMAvA9iW07hL/W8Aa4Cngd0rfZknnA6vKcufZ3ty/8iMiopuuQW/7ZkBdxhj4SIe+pcDS7aouIiLGLN+MjYioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKtf1xiOSlgLvATbafkOb/j8BTmys7/XAQLm71P3AE8BzwBbbg/0qPCIietPLEf2lwLxOnbY/bvsQ24cAZwI3Drtd4NGlPyEfETEJuga97ZuAXu/zuhC4bEwVRUREX/XtHL2kV9A68v9mo9nA9ZJulbS4X88VERG963qOfhTeC/zTsNM2b7O9QdI+wApJd5d3CNsoLwSLAWbNmtXHsiIidmz9vOpmAcNO29jeUP5uBK4G5nZa2PYS24O2BwcGBvpYVkTEjq0vQS9pD+BI4P822l4pafet08AxwPf78XwREdG7Xi6vvAw4CpguaT1wDrAzgO1LyrD/DFxv+2eNRV8FXC1p6/N8zfa3+1d6RET0omvQ217Yw5hLaV2G2WxbCxy8vYVFRER/5JuxERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5RL0ERGV6xr0kpZK2iip7f1eJR0l6TFJt5fH2Y2+eZLukbRG0hn9LDwiInrTyxH9pcC8LmP+0fYh5XEegKRpwMXAccCBwEJJB46l2IiIGL2uQW/7JmDzdqx7LrDG9lrbzwKXA/O3Yz0RETEG/TpH/1ZJ35N0raSDStt+wLrGmPWlrS1JiyUNSRratGlTn8qKiIh+BP1twP62Dwb+D3DN9qzE9hLbg7YHBwYG+lBWRERAH4Le9uO2nyzTy4GdJU0HNgAzG0NnlLaIiJhAYw56Sb8qSWV6blnnw8AqYI6kAyTtAiwAlo31+SIiYnR26jZA0mXAUcB0SeuBc4CdAWxfApwAnCppC/A0sMC2gS2STgOuA6YBS22vHpetiIiIjroGve2FXfovAi7q0LccWL59pUVERD/km7EREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZXrGvSSlkraKOn7HfpPlHSHpDsl3SLp4Ebf/aX9dklD/Sw8IiJ608sR/aXAvBH67wOOtP1G4HxgybD+o20fYntw+0qMiIix6OWesTdJmj1C/y2N2ZXAjLGXFRER/dLvc/QnA9c25g1cL+lWSYtHWlDSYklDkoY2bdrU57IiInZcXY/oeyXpaFpB/7ZG89tsb5C0D7BC0t22b2q3vO0llNM+g4OD7lddERE7ur4c0Ut6E/AFYL7th7e2295Q/m4Ergbm9uP5IiKid2MOekmzgKuAD9r+YaP9lZJ23zoNHAO0vXInIiLGT9dTN5IuA44CpktaD5wD7Axg+xLgbOBXgM9KAthSrrB5FXB1adsJ+Jrtb4/DNkRExAh6uepmYZf+U4BT2rSvBQ7edomIiJhI+WZsRETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlegp6SUslbZTU9p6vavmMpDWS7pD05kbfIkn3lseifhUeERG96fWI/lJg3gj9xwFzymMx8DkASXvTusfsYcBc4BxJe21vsRERMXo9Bb3tm4DNIwyZD3zZLSuBPSW9GjgWWGF7s+1HgBWM/IIRERF91vXm4D3aD1jXmF9f2jq1b0PSYlrvBpg1a9Z2FzL7jG+Navz9F7x7XNczmTptw2i3uV/jX0qm2rZNtXpibCb633PKfBhre4ntQduDAwMDk11OREQ1+hX0G4CZjfkZpa1Te0RETJB+Bf0y4EPl6pvDgcdsPwRcBxwjaa/yIewxpS0iIiZIT+foJV0GHAVMl7Se1pU0OwPYvgRYDvwWsAZ4Cvid0rdZ0vnAqrKq82yP9KFuRET0WU9Bb3thl34DH+nQtxRYOvrSIiKiH6bMh7ERETE+EvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlEvQREZVL0EdEVC5BHxFRuQR9RETlegp6SfMk3SNpjaQz2vR/StLt5fFDSY82+p5r9C3rZ/EREdFd11sJSpoGXAy8C1gPrJK0zPZdW8fYPr0x/qPAoY1VPG37kP6VHBERo9HLEf1cYI3ttbafBS4H5o8wfiFwWT+Ki4iIsesl6PcD1jXm15e2bUjaHzgA+E6j+eWShiStlPS+Tk8iaXEZN7Rp06YeyoqIiF70+8PYBcCVtp9rtO1vexD4beCvJL223YK2l9getD04MDDQ57IiInZcvQT9BmBmY35GaWtnAcNO29jeUP6uBW7gxefvIyJinPUS9KuAOZIOkLQLrTDf5uoZSb8O7AV8t9G2l6Rdy/R04AjgruHLRkTE+Ol61Y3tLZJOA64DpgFLba+WdB4wZHtr6C8ALrftxuKvBz4v6XlaLyoXNK/WiYiI8dc16AFsLweWD2s7e9j8uW2WuwV44xjqi4iIMco3YyMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKtdT0EuaJ+keSWskndGm/yRJmyTdXh6nNPoWSbq3PBb1s/iIiOiu660EJU0DLgbeBawHVkla1uber1fYPm3YsnsD5wCDgIFby7KP9KX6iIjoqpcj+rnAGttrbT8LXA7M73H9xwIrbG8u4b4CmLd9pUZExPboJej3A9Y15teXtuH+i6Q7JF0paeYol0XSYklDkoY2bdrUQ1kREdGLfn0Y+7fAbNtvonXU/qXRrsD2EtuDtgcHBgb6VFZERPQS9BuAmY35GaXtF2w/bPuZMvsF4C29LhsREeOrl6BfBcyRdICkXYAFwLLmAEmvbsweD/ygTF8HHCNpL0l7AceUtoiImCBdr7qxvUXSabQCehqw1PZqSecBQ7aXAX8g6XhgC7AZOKksu1nS+bReLADOs715HLYjIiI66Br0ALaXA8uHtZ3dmD4TOLPDskuBpWOoMSIixiDfjI2IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFyCPiKicgn6iIjKJegjIiqXoI+IqFxPQS9pnqR7JK2RdEab/j+SdJekOyT9vaT9G33PSbq9PJYNXzYiIsZX11sJSpoGXAy8C1gPrJK0zPZdjWH/CgzafkrSqcCFwPtL39O2D+lz3RER0aNejujnAmtsr7X9LHA5ML85wPY/2H6qzK4EZvS3zIiI2F69BP1+wLrG/PrS1snJwLWN+ZdLGpK0UtL7Oi0kaXEZN7Rp06YeyoqIiF50PXUzGpI+AAwCRzaa97e9QdJrgO9IutP2j4Yva3sJsARgcHDQ/awrImJH1ssR/QZgZmN+Rml7EUnvBP4MON72M1vbbW8of9cCNwCHjqHeiIgYpV6CfhUwR9IBknYBFgAvunpG0qHA52mF/MZG+16Sdi3T04EjgOaHuBERMc66nrqxvUXSacB1wDRgqe3Vks4DhmwvAz4O7AZ8QxLAv9k+Hng98HlJz9N6Ublg2NU6ERExzno6R297ObB8WNvZjel3dljuFuCNYykwIiLGJt+MjYioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXII+IqJyCfqIiMol6CMiKpegj4ioXE9BL2mepHskrZF0Rpv+XSVdUfr/WdLsRt+Zpf0eScf2r/SIiOhF16CXNA24GDgOOBBYKOnAYcNOBh6x/WvAp4C/LMseSOtm4gcB84DPlvVFRMQE6eWIfi6wxvZa288ClwPzh42ZD3ypTF8JvEOtu4TPBy63/Yzt+4A1ZX0RETFBZHvkAdIJwDzbp5T5DwKH2T6tMeb7Zcz6Mv8j4DDgXGCl7a+U9i8C19q+ss3zLAYWl9n/ANwztk2bEqYDP53sIqa47KPeZD91t6Pvo/1tD7Tr2GmiK+nE9hJgyWTX0U+ShmwPTnYdU1n2UW+yn7rLPuqsl1M3G4CZjfkZpa3tGEk7AXsAD/e4bEREjKNegn4VMEfSAZJ2ofXh6rJhY5YBi8r0CcB33DontAxYUK7KOQCYA/xLf0qPiIhedD11Y3uLpNOA64BpwFLbqyWdBwzZXgZ8EfgbSWuAzbReDCjjvg7cBWwBPmL7uXHalqmoqlNR4yT7qDfZT91lH3XQ9cPYiIh4acs3YyMiKpegj4ioXIK+TyQtlbSxfKdga9vHJd0t6Q5JV0vaczJrnGzt9lGj748lWdL0yahtKum0nyR9tPz3tFrShZNV31TQ4f+3QyStlHS7pCFJ+XJmkaDvn0tp/cxD0wrgDbbfBPwQOHOii5piLmXbfYSkmcAxwL9NdEFT1KUM20+Sjqb1TfODbR8EfGIS6ppKLmXb/5YuBP6X7UOAs8t8kKDvG9s30briqNl2ve0tZXYlre8R7LDa7aPiU8CfArkygI776VTgAtvPlDEbJ7ywKaTDPjLwy2V6D+DBCS1qCkvQT5zfBa6d7CKmGknzgQ22vzfZtUxxrwPeXn4d9kZJ/3GyC5qC/hD4uKR1tN7x7OjvoH8hQT8BJP0Zre8RfHWya5lKJL0COIvW2+wY2U7A3sDhwJ8AXy8/HBgvOBU43fZM4HRa3+8JEvTjTtJJwHuAE50vLQz3WuAA4HuS7qd1aus2Sb86qVVNTeuBq9zyL8DztH7EK16wCLiqTH+D/FLuLyTox5GkebTOPR9v+6nJrmeqsX2n7X1sz7Y9m1aYvdn2jye5tKnoGuBoAEmvA3Zhx/6lxnYeBI4s078J3DuJtUwpU+bXK1/qJF0GHAVMl7QeOIfWOcJdgRXlXfZK278/aUVOsnb7yHbeXg/T4b+lpcDScjnhs8CiHfkdYod99HvAp8sPK/6cF372fIeXn0CIiKhcTt1ERFQuQR8RUbkEfURE5RL0ERGVS9BHRFQuQR8RUbkEfURE5f4/2KdejJ5bnhEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1laK6IJaG6DX",
        "outputId": "8a92f23d-2331-45dd-f9ec-6768c080b391"
      },
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "max_length = 10.#in secs\n",
        "max_samples =  max_length*SAMPLING_RATE\n",
        "for i in range(len(X)):\n",
        "\n",
        "  temp = X[i]\n",
        "  #temp = librosa.resample(temp, 20000, 16000)\n",
        "  temp = np.reshape(temp,(1,temp.shape[0]))  \n",
        "  \n",
        "  if temp.shape[1] < max_samples:\n",
        "    \n",
        "    offset = max_samples - len(temp)\n",
        "\n",
        "    shape = np.shape(temp)\n",
        "    tt = np.zeros((1,int(max_samples)))\n",
        "    tt[:shape[0],:shape[1]] = temp\n",
        "\n",
        "    temp = tt\n",
        "    \n",
        "     \n",
        "  if temp.shape[1] > max_samples:\n",
        "    temp = temp[0,:int((max_samples))]\n",
        "    temp = np.reshape(temp,(1,temp.shape[0]))  \n",
        "  X[i] = temp\n",
        "  \n",
        "  print(temp.shape)\n",
        "lens = []\n",
        "for it in X:\n",
        "  lens.append(it.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "print('Mean secs')\n",
        "print(np.mean(lens)/SAMPLING_RATE)\n",
        "print('Max secs')\n",
        "print(np.max(lens)/SAMPLING_RATE)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "(1, 220500)\n",
            "Mean secs\n",
            "10.0\n",
            "Max secs\n",
            "10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olGy2H9GG-zL"
      },
      "source": [
        "import math\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "rand = random.Random(42)\n",
        "#Dentamaro et al.\n",
        "def inter_patient_scheme(X_,lbls_,filenames, test_size = 0.2):\n",
        "  people_index = {}\n",
        "  people_class = {}\n",
        "  X = copy.deepcopy(X_)\n",
        "  lbls = copy.deepcopy(lbls_)\n",
        "  \n",
        "  for i in range(len(filenames)):\n",
        "        \n",
        "      usercode = filenames[i]\n",
        "      if not usercode in people_class:\n",
        "        people_class[usercode] = lbls[i]\n",
        "      #print(usercode)\n",
        "      if usercode in people_index:\n",
        "        people_index[usercode].append(i)\n",
        "      else:\n",
        "        people_index[usercode] = []\n",
        "        people_index[usercode].append(i)\n",
        "  #print(people_index)    \n",
        "  keys = list(people_index.keys())\n",
        "\n",
        "  rand.shuffle(keys)\n",
        "  #print(keys) \n",
        "  \n",
        "\n",
        "  peoples_in_train = math.floor(len(keys)*(1.-test_size))\n",
        "  people_in_test = len(keys)-peoples_in_train\n",
        "  temp_classes = []\n",
        "  j = 0\n",
        "  for k in keys:\n",
        "    if j < peoples_in_train:\n",
        "      temp_classes.append(people_class[k])\n",
        "    j += 1\n",
        "\n",
        "  unique, counts = np.unique(temp_classes, return_counts=True)\n",
        "  min_index = np.where(counts == np.min(counts))\n",
        "  min_people = np.min(counts)\n",
        "  min_class = unique[min_index]\n",
        "  print('minority class '+str(min_class))\n",
        "  print('minority people '+str(min_people))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #print(peoples_in_train)\n",
        "  training_items = []\n",
        "  testing_items = []\n",
        "\n",
        "  class_counter = {}\n",
        "\n",
        "  train_usercodes = []\n",
        "  test_usercodes = []\n",
        "  #per people balance \n",
        "  for j in range(peoples_in_train):\n",
        "    index = people_index[keys[j]]\n",
        "    \n",
        "    this_class = people_class[keys[j]]\n",
        "    #print(this_class)\n",
        "    #print(class_counter)\n",
        "    if not this_class in class_counter:\n",
        "      class_counter[this_class] = 0\n",
        "    \n",
        "    if this_class != min_class and class_counter[this_class] < min_people:      \n",
        "      for h in index:\n",
        "        training_items.append(h)\n",
        "        train_usercodes.append(keys[j])\n",
        "      class_counter[this_class] += 1\n",
        "    elif this_class != min_class and class_counter[this_class] >= min_people :\n",
        "       for h in index:\n",
        "          pass\n",
        "          #testing_items.append(h)\n",
        "       #class_counter[this_class] += 1\n",
        "    else:\n",
        "      for h in index:\n",
        "        training_items.append(h)\n",
        "        train_usercodes.append(keys[j])\n",
        "      class_counter[this_class] += 1\n",
        "  \n",
        "  temp_classes = []\n",
        "  j = 0\n",
        "  for k in keys:\n",
        "    if j >= peoples_in_train:\n",
        "      temp_classes.append(people_class[k])\n",
        "    j += 1\n",
        "\n",
        "  #print(temp_classes)\n",
        "  class_counter = {}\n",
        "  unique, counts = np.unique(temp_classes, return_counts=True)\n",
        "  min_index = np.where(counts == np.min(counts))\n",
        "  min_people = np.min(counts)\n",
        "  min_class = unique[min_index]\n",
        "  #print(min_people)\n",
        "  #print(min_class)\n",
        "  for j in range(peoples_in_train, len(keys)):\n",
        "    index = people_index[keys[j]]\n",
        "    \n",
        "    this_class = people_class[keys[j]]\n",
        "    #print(this_class)\n",
        "    #print(class_counter)\n",
        "    if not this_class in class_counter:\n",
        "      class_counter[this_class] = 0\n",
        "    \n",
        "    if this_class != min_class[0] and class_counter[this_class] < min_people:      \n",
        "      for h in index:\n",
        "        testing_items.append(h)\n",
        "        test_usercodes.append(keys[j])\n",
        "        #print('Negative index --> '+str(h))\n",
        "      \n",
        "      class_counter[this_class] += 1\n",
        "    elif this_class == min_class[0]:\n",
        "      for h in index:\n",
        "        testing_items.append(h)\n",
        "        test_usercodes.append(keys[j])\n",
        "        #print('Positive index --> '+str(h))\n",
        "    \n",
        "  \n",
        "\n",
        "  \n",
        "  testing_items = np.asarray(testing_items)\n",
        "  training_items = np.asarray(training_items)\n",
        "  #lbls = np.asarray(lbls)\n",
        "  yy = to_categorical(lbls)\n",
        "  X_train = X[training_items,:]\n",
        "  y_train = yy[training_items,:]\n",
        "  X_test = X[testing_items,:]\n",
        "  y_test = yy[testing_items,:]\n",
        "  #print(y_test)\n",
        "  ffn = np.asarray(filenames)[training_items]\n",
        "\n",
        "  #for j in range(len(lbls)):\n",
        "    #print(str(yy[j])+ ' --> '+str(lbls[j]))\n",
        "\n",
        "  return X_train, X_test, y_train, y_test,train_usercodes, test_usercodes\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Conv1D, Add, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "from tf_siren import SinusodialRepresentationDense, Sine\n",
        "def attach_attention_module(net, attention_module, activation=Activation('relu'), ratio = 8):\n",
        "  if attention_module == 'se_block': # SE_block\n",
        "    net = se_block(net, activation=activation, ratio=ratio)\n",
        "  elif attention_module == 'cbam_block': # CBAM_block\n",
        "    net = cbam_block(net, activation=activation, ratio=ratio)\n",
        "  elif attention_module == 'dbam_block': # CBAM_block\n",
        "    net = dbam_block(net, activation=activation)\n",
        "  elif attention_module == 'eca_block': # CBAM_block\n",
        "    net = ecanet(net)\n",
        "  else:\n",
        "    pass\n",
        "  return net\n",
        "\n",
        "def se_block(input_feature, ratio=8, activation='relu'):\n",
        "\t\"\"\"Contains the implementation of Squeeze-and-Excitation(SE) block.\n",
        "\tAs described in https://arxiv.org/abs/1709.01507.\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\tchannel = input_feature.shape[channel_axis]\n",
        "\n",
        "\tse_feature = GlobalAveragePooling2D()(input_feature)\n",
        "\tse_feature = Reshape((1, 1, channel))(se_feature)\n",
        "\tassert se_feature.shape[1:] == (1,1,channel)\n",
        "\n",
        "  \n",
        "\tse_feature = Dense(channel // ratio,\n",
        "\t\t\t\t\t   kernel_initializer='he_normal',\n",
        "\t\t\t\t\t   use_bias=True,\n",
        "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
        "\tse_feature  = Activation(activation)(se_feature)\n",
        "\n",
        "\n",
        "\t#se_feature = SinusodialRepresentationDense(channel // ratio,activation='sine', w0=1.0)(se_feature)#72 auc\n",
        "\tassert se_feature.shape[1:] == (1,1,channel//ratio)\n",
        "\tse_feature = Dense(channel,\n",
        "\t\t\t\t\t   activation='sigmoid',\n",
        "\t\t\t\t\t   kernel_initializer='he_normal',\n",
        "\t\t\t\t\t   use_bias=True,\n",
        "\t\t\t\t\t   bias_initializer='zeros')(se_feature)\n",
        "\tassert se_feature.shape[1:] == (1,1,channel)\n",
        "\tif K.image_data_format() == 'channels_first':\n",
        "\t\tse_feature = Permute((3, 1, 2))(se_feature)\n",
        "\n",
        "\tse_feature = multiply([input_feature, se_feature])\n",
        "\treturn se_feature\n",
        "def dbam_block(cbam_feature, ratio=8, activation='relu'):\n",
        "\n",
        "\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
        "\tAs described in https://arxiv.org/abs/1807.06521.\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tcbam_feature = se_block(cbam_feature, ratio)#channel_attention(cbam_feature, ratio)\n",
        "\tcbam_feature = spatial_attention(cbam_feature)\n",
        "\treturn cbam_feature\n",
        "\t \n",
        "def cbam_block(cbam_feature, ratio=8, activation='relu'):\n",
        "\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
        "\tAs described in https://arxiv.org/abs/1807.06521.\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
        "\tcbam_feature = spatial_attention(cbam_feature)\n",
        "\treturn cbam_feature\n",
        "\t \n",
        "\n",
        "def ecanet(input_feature,gamma=2,b=1,):\n",
        "  channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "  channels = input_feature.shape[channel_axis]\n",
        "  t = int(abs((math.log(channels,2)+b)/gamma))\n",
        "  k = t if t%2 else t+1\n",
        "  x_global_avg_pool = GlobalAveragePooling2D()(input_feature)\n",
        "  x = Reshape((channels,1))(x_global_avg_pool)\n",
        "  x = Conv1D(1,kernel_size=k,padding=\"same\")(x)\n",
        "  x = Activation('sigmoid')(x)  #shape=[batch,chnnels,1]\n",
        "  x = Reshape((1, 1, channels))(x)\n",
        "  output = multiply([input_feature,x])\n",
        "  return output\n",
        "\n",
        "\n",
        "def channel_attention(input_feature, ratio=8,activation='relu'):\n",
        "\t\n",
        "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\tchannel = input_feature.shape[channel_axis]\n",
        "\t\n",
        "\tshared_layer_one = Dense(channel//ratio,\n",
        "\t\t\t\t\t\t\t activation=activation,\n",
        "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
        "\t\t\t\t\t\t\t use_bias=True,\n",
        "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
        "\tshared_layer_two = Dense(channel,\n",
        "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
        "\t\t\t\t\t\t\t use_bias=True,\n",
        "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
        "\t\n",
        "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
        "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
        "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
        "\tavg_pool = shared_layer_one(avg_pool)\n",
        "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
        "\tavg_pool = shared_layer_two(avg_pool)\n",
        "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
        "\t\n",
        "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
        "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
        "\tassert max_pool.shape[1:] == (1,1,channel)\n",
        "\tmax_pool = shared_layer_one(max_pool)\n",
        "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
        "\tmax_pool = shared_layer_two(max_pool)\n",
        "\tassert max_pool.shape[1:] == (1,1,channel)\n",
        "\t\n",
        "\tcbam_feature = Add()([avg_pool,max_pool])\n",
        "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
        "\t\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
        "\t\n",
        "\treturn multiply([input_feature, cbam_feature])\n",
        "\n",
        "def spatial_attention(input_feature,activation='relu'):\n",
        "\tkernel_size = 7\n",
        "\t\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tchannel = input_feature.shape[1]\n",
        "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
        "\telse:\n",
        "\t\tchannel = input_feature.shape[-1]\n",
        "\t\tcbam_feature = input_feature\n",
        "\t\n",
        "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
        "\tassert avg_pool.shape[-1] == 1\n",
        "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
        "\tassert max_pool.shape[-1] == 1\n",
        "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
        "\tassert concat.shape[-1] == 2\n",
        "\tcbam_feature = Conv2D(filters = 1,\n",
        "\t\t\t\t\tkernel_size=kernel_size,\n",
        "\t\t\t\t\tstrides=1,\n",
        "\t\t\t\t\tpadding='same',\n",
        "\t\t\t\t\tactivation='sigmoid',\n",
        "\t\t\t\t\tkernel_initializer='he_normal',\n",
        "\t\t\t\t\tuse_bias=False)(concat)\t\n",
        "\tassert cbam_feature.shape[-1] == 1\n",
        "\t\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
        "\t\t\n",
        "\treturn multiply([input_feature, cbam_feature])\n",
        "\t\t\n",
        "\"\"\"Backend operations of Kapre.\n",
        "\n",
        "This module summarizes operations and functions that are used in Kapre layers.\n",
        "\n",
        "Attributes:\n",
        "    _CH_FIRST_STR (str): 'channels_first', a pre-defined string.\n",
        "    _CH_LAST_STR (str): 'channels_last', a pre-defined string.\n",
        "    _CH_DEFAULT_STR (str): 'default', a pre-defined string.\n",
        "\n",
        "\"\"\"\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow\n",
        "#tensorflow.random.set_seed(42)\n",
        "\n",
        "_CH_FIRST_STR = 'channels_first'\n",
        "_CH_LAST_STR = 'channels_last'\n",
        "_CH_DEFAULT_STR = 'default'\n",
        "\n",
        "\n",
        "def get_window_fn(window_name=None):\n",
        "    \"\"\"Return a window function given its name.\n",
        "    This function is used inside layers such as `STFT` to get a window function.\n",
        "\n",
        "    Args:\n",
        "        window_name (None or str): name of window function. On Tensorflow 2.3, there are five windows available in\n",
        "        `tf.signal` (`hamming_window`, `hann_window`, `kaiser_bessel_derived_window`, `kaiser_window`, `vorbis_window`).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if window_name is None:\n",
        "        return tf.signal.hann_window\n",
        "\n",
        "    available_windows = {\n",
        "        'hamming_window': tf.signal.hamming_window,\n",
        "        'hann_window': tf.signal.hann_window,\n",
        "    }\n",
        "    if hasattr(tf.signal, 'kaiser_bessel_derived_window'):\n",
        "        available_windows['kaiser_bessel_derived_window'] = tf.signal.kaiser_bessel_derived_window\n",
        "    if hasattr(tf.signal, 'kaiser_window'):\n",
        "        available_windows['kaiser_window'] = tf.signal.kaiser_window\n",
        "    if hasattr(tf.signal, 'vorbis_window'):\n",
        "        available_windows['vorbis_window'] = tf.signal.vorbis_window\n",
        "\n",
        "    if window_name not in available_windows:\n",
        "        raise NotImplementedError(\n",
        "            'Window name %s is not supported now. Currently, %d windows are'\n",
        "            'supported - %s'\n",
        "            % (\n",
        "                window_name,\n",
        "                len(available_windows),\n",
        "                ', '.join([k for k in available_windows.keys()]),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return available_windows[window_name]\n",
        "\n",
        "\n",
        "def validate_data_format_str(data_format):\n",
        "    \"\"\"A function that validates the data format string.\"\"\"\n",
        "    if data_format not in (_CH_DEFAULT_STR, _CH_FIRST_STR, _CH_LAST_STR):\n",
        "        raise ValueError(\n",
        "            'data_format should be one of {}'.format(\n",
        "                str([_CH_FIRST_STR, _CH_LAST_STR, _CH_DEFAULT_STR])\n",
        "            )\n",
        "            + ' but we received {}'.format(data_format)\n",
        "        )\n",
        "\n",
        "\n",
        "def magnitude_to_decibel(x, ref_value=1.0, amin=1e-5, dynamic_range=80.0):\n",
        "    \"\"\"A function that converts magnitude to decibel scaling.\n",
        "    In essence, it runs `10 * log10(x)`, but with some other utility operations.\n",
        "\n",
        "    Similar to `librosa.amplitude_to_db` with `ref=1.0` and `top_db=dynamic_range`\n",
        "\n",
        "    Args:\n",
        "        x (`Tensor`): float tensor. Can be batch or not. Something like magnitude of STFT.\n",
        "        ref_value (`float`): an input value that would become 0 dB in the result.\n",
        "            For spectrogram magnitudes, ref_value=1.0 usually make the decibel-scaled output to be around zero\n",
        "            if the input audio was in [-1, 1].\n",
        "        amin (`float`): the noise floor of the input. An input that is smaller than `amin`, it's converted to `amin`.\n",
        "        dynamic_range (`float`): range of the resulting value. E.g., if the maximum magnitude is 30 dB,\n",
        "            the noise floor of the output would become (30 - dynamic_range) dB\n",
        "\n",
        "    Returns:\n",
        "        log_spec (`Tensor`): a decibel-scaled version of `x`.\n",
        "\n",
        "    Note:\n",
        "        In many deep learning based application, the input spectrogram magnitudes (e.g., abs(STFT)) are decibel-scaled\n",
        "        (=logarithmically mapped) for a better performance.\n",
        "\n",
        "    Example:\n",
        "        ::\n",
        "\n",
        "            input_shape = (2048, 1)  # mono signal\n",
        "            model = Sequential()\n",
        "            model.add(kapre.Frame(frame_length=1024, hop_length=512, input_shape=input_shape))\n",
        "            # now the shape is (batch, n_frame=3, frame_length=1024, ch=1)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def _log10(x):\n",
        "        return tf.math.log(x) / tf.math.log(tf.constant(10, dtype=x.dtype))\n",
        "\n",
        "    if K.ndim(x) > 1:  # we assume x is batch in this case\n",
        "        max_axis = tuple(range(K.ndim(x))[1:])\n",
        "    else:\n",
        "        max_axis = None\n",
        "\n",
        "    if amin is None:\n",
        "        amin = 1e-5\n",
        "\n",
        "    log_spec = 10.0 * _log10(tf.math.maximum(x, amin))\n",
        "    log_spec = log_spec - 10.0 * _log10(tf.math.maximum(amin, ref_value))\n",
        "\n",
        "    log_spec = tf.math.maximum(\n",
        "        log_spec, tf.math.reduce_max(log_spec, axis=max_axis, keepdims=True) - dynamic_range\n",
        "    )\n",
        "\n",
        "    return log_spec\n",
        "\n",
        "\n",
        "def filterbank_mel(\n",
        "    sample_rate, n_freq, n_mels=128, f_min=0.0, f_max=None, htk=False, norm='slaney',trainable=False, num_classes=2\n",
        "):\n",
        "    \"\"\"A wrapper for librosa.filters.mel that additionally does transpose and tensor conversion\n",
        "\n",
        "    Args:\n",
        "        sample_rate (`int`): sample rate of the input audio\n",
        "        n_freq (`int`): number of frequency bins in the input STFT magnitude.\n",
        "        n_mels (`int`): the number of mel bands\n",
        "        f_min (`float`): lowest frequency that is going to be included in the mel filterbank (Hertz)\n",
        "        f_max (`float`): highest frequency that is going to be included in the mel filterbank (Hertz)\n",
        "        htk (bool): whether to use `htk` formula or not\n",
        "        norm: The default, 'slaney', would normalize the the mel weights by the width of the mel band.\n",
        "\n",
        "    Returns:\n",
        "        (`Tensor`): mel filterbanks. Shape=`(n_freq, n_mels)`\n",
        "    \"\"\"\n",
        "\n",
        "    filterbank = librosa.filters.mel(\n",
        "        sr=sample_rate,\n",
        "        n_fft=(n_freq - 1) * 2,\n",
        "        n_mels=n_mels,\n",
        "        fmin=f_min,\n",
        "        fmax=f_max,\n",
        "        htk=htk,\n",
        "        norm=norm,\n",
        "    ).astype(K.floatx())\n",
        "\n",
        "    ff = filterbank.T\n",
        "    print('FF shape',ff.shape)\n",
        "    \n",
        "    if trainable:\n",
        "      variables = []\n",
        "      for i in range(num_classes):\n",
        "        #variables.append(tf.Variable(tf.ones(shape=(ff.shape[0],ff.shape[1]), dtype=tf.float32),trainable=True, name='kernel_variable_'+str(i))*tf.Variable(ff,trainable=False))\n",
        "        variables.append(tf.Variable(ff,trainable=True))\n",
        "\n",
        "      \n",
        "      ff = tf.add_n(variables)# / num_classes #* tf.Variable(tf.convert_to_tensor(ff),trainable=False)\n",
        "      \n",
        "\n",
        "    print('Trainable mel spectrogram is '+str(trainable))\n",
        "    return ff\n",
        "\n",
        "\n",
        "def filterbank_log(sample_rate, n_freq, n_bins=84, bins_per_octave=12, f_min=None, spread=0.125, trainable = False):\n",
        "    \"\"\"A function that returns a approximation of constant-Q filter banks for a fixed-window STFT.\n",
        "    Each filter is a log-normal window centered at the corresponding frequency.\n",
        "\n",
        "    Args:\n",
        "        sample_rate (`int`): audio sampling rate\n",
        "        n_freq (`int`): number of the input frequency bins. E.g., `n_fft / 2 + 1`\n",
        "        n_bins (`int`): number of the resulting log-frequency bins.  Defaults to 84 (7 octaves).\n",
        "        bins_per_octave (`int`): number of bins per octave. Defaults to 12 (semitones).\n",
        "        f_min (`float`): lowest frequency that is going to be included in the log filterbank. Defaults to `C1 ~= 32.70`\n",
        "        spread (`float`): spread of each filter, as a fraction of a bin.\n",
        "\n",
        "    Returns:\n",
        "        (`Tensor`): log-frequency filterbanks. Shape=`(n_freq, n_bins)`\n",
        "\n",
        "    Note:\n",
        "        The code is originally from `logfrequency` in librosa 0.4 (deprecated) and copy-and-pasted.\n",
        "        `tuning` parameter was removed and we use `n_freq` instead of `n_fft`.\n",
        "    \"\"\"\n",
        "\n",
        "    if f_min is None:\n",
        "        f_min = 32.70319566\n",
        "\n",
        "    f_max = f_min * 2 ** (n_bins / bins_per_octave)\n",
        "    if f_max > sample_rate // 2:\n",
        "        raise RuntimeError(\n",
        "            'Maximum frequency of log filterbank should be lower or equal to the maximum'\n",
        "            'frequency of the input (defined by its sample rate), '\n",
        "            'but f_max=%f and maximum frequency is %f. \\n'\n",
        "            'Fix it by reducing n_bins, increasing bins_per_octave and/or reducing f_min.\\n'\n",
        "            'You can also do it by increasing sample_rate but it means you need to upsample'\n",
        "            'the input audio data, too.' % (f_max, sample_rate)\n",
        "        )\n",
        "\n",
        "    # What's the shape parameter for our log-normal filters?\n",
        "    sigma = float(spread) / bins_per_octave\n",
        "\n",
        "    # Construct the output matrix\n",
        "    basis = np.zeros((n_bins, n_freq))\n",
        "\n",
        "    # Get log frequencies of bins\n",
        "    log_freqs = np.log2(librosa.fft_frequencies(sample_rate, (n_freq - 1) * 2)[1:])\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        # What's the center (median) frequency of this filter?\n",
        "        c_freq = f_min * (2.0 ** (float(i) / bins_per_octave))\n",
        "\n",
        "        # Place a log-normal window around c_freq\n",
        "        basis[i, 1:] = np.exp(\n",
        "            -0.5 * ((log_freqs - np.log2(c_freq)) / sigma) ** 2 - np.log2(sigma) - log_freqs\n",
        "        )\n",
        "\n",
        "    # Normalize the filters\n",
        "    basis = librosa.util.normalize(basis, norm=1, axis=1)\n",
        "    basis = basis.astype(K.floatx())\n",
        "    print('Trainable mel spectrogram is '+str(trainable))\n",
        "    return tf.Variable(tf.convert_to_tensor(basis.T), trainable=trainable)\n",
        "\n",
        "\n",
        "def mu_law_encoding(signal, quantization_channels):\n",
        "    \"\"\"Encode signal based on mu-law companding. Also called mu-law compressing.\n",
        "\n",
        "    This algorithm assumes the signal has been scaled to between -1 and 1 and returns a signal encoded\n",
        "    with values from 0 to quantization_channels - 1.\n",
        "    See `Wikipedia <https://en.wikipedia.org/wiki/Μ-law_algorithm>`_ for more details.\n",
        "\n",
        "    Args:\n",
        "        signal (float `Tensor`): audio signal to encode\n",
        "        quantization_channels (positive int): Number of channels. For 8-bit encoding, use 256.\n",
        "\n",
        "    Returns:\n",
        "        signal_mu (int `Tensor`): mu-encoded signal\n",
        "    \"\"\"\n",
        "    mu = quantization_channels - 1.0\n",
        "    signal_mu = tf.math.sign(signal) * tf.math.log1p(mu * tf.math.abs(signal)) / tf.math.log1p(mu)\n",
        "    signal_mu = tf.cast(((signal_mu + 1) / 2.0 * mu + 0.5), tf.int32)\n",
        "    return signal_mu\n",
        "\n",
        "\n",
        "def mu_law_decoding(signal_mu, quantization_channels):\n",
        "    \"\"\"Decode mu-law encoded signals based on mu-law companding. Also called mu-law expanding.\n",
        "\n",
        "    See `Wikipedia <https://en.wikipedia.org/wiki/Μ-law_algorithm>`_ for more details.\n",
        "\n",
        "    Args:\n",
        "        signal_mu (int `Tensor`): mu-encoded signal to decode\n",
        "        quantization_channels (positive int): Number of channels. For 8-bit encoding, use 256.\n",
        "\n",
        "    Returns:\n",
        "        signal (float `Tensor`): decoded audio signal\n",
        "    \"\"\"\n",
        "    mu = quantization_channels - 1.0\n",
        "    signal_mu = K.cast_to_floatx(signal_mu)\n",
        "\n",
        "    signal = (signal_mu / mu) * 2 - 1.0\n",
        "    signal = (\n",
        "        tf.math.sign(signal) * (tf.math.exp(tf.math.abs(signal) * tf.math.log1p(mu)) - 1.0) / mu\n",
        "    )\n",
        "    return signal\n",
        "\n",
        "\n",
        "\n",
        "class ApplyFilterbank(tensorflow.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Apply a filterbank to the input spectrograms.\n",
        "    Args:\n",
        "        filterbank (`Tensor`): filterbank tensor in a shape of (n_freq, n_filterbanks)\n",
        "        data_format (`str`): specifies the data format of batch input/output\n",
        "        **kwargs: Keyword args for the parent keras layer (e.g., `name`)\n",
        "    Example:\n",
        "        ::\n",
        "            input_shape = (2048, 1)  # mono signal\n",
        "            n_fft = 1024\n",
        "            n_hop = n_fft // 2\n",
        "            kwargs = {\n",
        "                'sample_rate': 22050,\n",
        "                'n_freq': n_fft // 2 + 1,\n",
        "                'n_mels': 128,\n",
        "                'f_min': 0.0,\n",
        "                'f_max': 8000,\n",
        "            }\n",
        "            model = Sequential()\n",
        "            model.add(kapre.STFT(n_fft=n_fft, hop_length=n_hop, input_shape=input_shape))\n",
        "            model.add(Magnitude())\n",
        "            # (batch, n_frame=3, n_freq=n_fft // 2 + 1, ch=1) and dtype is float\n",
        "            model.add(ApplyFilterbank(type='mel', filterbank_kwargs=kwargs))\n",
        "            # (batch, n_frame=3, n_mels=128, ch=1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, type, filterbank_kwargs, data_format='default', **kwargs,\n",
        "    ):\n",
        "\n",
        "        kapre.backend.validate_data_format_str(data_format)\n",
        "\n",
        "        self.type = type\n",
        "        self.filterbank_kwargs = filterbank_kwargs\n",
        "\n",
        "        if type == 'log':\n",
        "            self.filterbank = _log_filterbank = filterbank_log(**filterbank_kwargs)\n",
        "        elif type == 'mel':\n",
        "            self.filterbank = _mel_filterbank = filterbank_mel(**filterbank_kwargs)\n",
        "\n",
        "        if data_format == _CH_DEFAULT_STR:\n",
        "            self.data_format = K.image_data_format()\n",
        "        else:\n",
        "            self.data_format = data_format\n",
        "\n",
        "        if self.data_format == _CH_FIRST_STR:\n",
        "            self.freq_axis = 3\n",
        "        else:\n",
        "            self.freq_axis = 2\n",
        "        super(ApplyFilterbank, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Apply filterbank to `x`.\n",
        "        Args:\n",
        "            x (`Tensor`): float tensor in 2D batch shape.\n",
        "        \"\"\"\n",
        "\n",
        "        # x: 2d batch input. (b, t, fr, ch) or (b, ch, t, fr)\n",
        "        output = tf.tensordot(x, self.filterbank, axes=(self.freq_axis, 0))\n",
        "        # ch_last -> (b, t, ch, new_fr). ch_first -> (b, ch, t, new_fr)\n",
        "        if self.data_format == _CH_LAST_STR:\n",
        "            output = tf.transpose(output, (0, 1, 3, 2))\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ApplyFilterbank, self).get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                'type': self.type,\n",
        "                'filterbank_kwargs': self.filterbank_kwargs,\n",
        "                'data_format': self.data_format,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "#AUCORESNET V2 \n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense, Lambda, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D\n",
        "from tensorflow.keras.layers import Concatenate, Add\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, LambdaCallback, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import os\n",
        "from tf_siren import SinusodialRepresentationDense, Sine\n",
        "from matplotlib import pyplot as plt\n",
        "from kapre.time_frequency import (\n",
        "    STFT,\n",
        "    InverseSTFT,\n",
        "    Phase,\n",
        "    MagnitudeToDecibel, \n",
        "    ConcatenateFrequencyMap,\n",
        "    Magnitude\n",
        ")\n",
        "import kapre\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential, Model\n",
        "#from spela.melspectrogram import Melspectrogram, Spectrogram\n",
        "\n",
        "import math\n",
        "class MagnitudeTR(Layer):\n",
        "\n",
        "  def __init__(self, trainable, **kwargs):\n",
        "    super(MagnitudeTR, self).__init__(**kwargs)\n",
        "    self.trainable = trainable\n",
        "\n",
        "    \"\"\"Compute the magnitude of the complex input, resulting in a float tensor\n",
        "    Example:\n",
        "        ::\n",
        "            input_shape = (2048, 1)  # mono signal\n",
        "            model = Sequential()\n",
        "            model.add(kapre.STFT(n_fft=1024, hop_length=512, input_shape=input_shape))\n",
        "            mode.add(Magnitude())\n",
        "            # now the shape is (batch, n_frame=3, n_freq=513, ch=1) and dtype is float\n",
        "    \"\"\"\n",
        "  def build(self, input_shape):\n",
        "        \n",
        "        self.u = self.add_weight(  shape=(input_shape[3], 1), initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        #self.kernel = self.add_weight(name='kernel',  initializer=tf.keras.initializers.GlorotUniform(), \n",
        "        #                               trainable=True,\n",
        "                                      #shape=(input_shape[-1], input_shape[-2]))\n",
        "        #                              shape=( input_shape[3], input_shape[2]))\n",
        "        super(MagnitudeTR, self).build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (complex `Tensor`): input complex tensor\n",
        "        Returns:\n",
        "            (float `Tensor`): magnitude of `x`\n",
        "        \"\"\"\n",
        "        xr = tf.math.real(x)\n",
        "        #xr = tf.reshape(xr,(tf.shape(xr)[0],tf.shape(xr)[1]))\n",
        "        if self.trainable:\n",
        "          xr = K.dot(xr,self.u)\n",
        "          #xr = tf.reshape(xr,(tf.shape(xr)[1],tf.shape(xr)[0], 1))\n",
        "\n",
        "        #tf.shape(xr)\n",
        "        #xr = xr.numpy()\n",
        "        xi = tf.math.imag(x)\n",
        "        #xr1 = tf.Variable(xr,trainable=self.trainable)\n",
        "        xb = tf.complex(xr,xi)\n",
        "        #x = tf.Variable(lambda:tf.math.abs(x),trainable=self.trainable)\n",
        "        \n",
        "        return tf.abs(xb)\n",
        "        #return tf.abs(x)\n",
        "#from tensorflow.keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "#get_custom_objects().update({'custom_activation': SineReLU()})\n",
        "def sinc(x):\n",
        "    tf.keras.activations.swish(x)\n",
        "    #return  K.exp(-K.pow(x,2))\n",
        "    #x = tf.where(tf.abs(x) < 1e-20, 1e-20 * tf.ones_like(x), x)\n",
        "    #return tf.sin(x) / x\n",
        "\n",
        "class AUCOResNetV2:\n",
        "    def __init__(self, att='att2', gmode='concat', compatibilityfunction='pc', datasetname=\"covid\", input_shape=(1,64000),\n",
        "                 outputclasses=100, weight_decay=0.0005, optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy', 'AUC'],\n",
        "                 runs = [1, 8, 3, 5, 4], n_fft=1024,sample_rate=22050,n_mels=128, win_length =160, hop_length=344, \n",
        "                 return_decibel=True,input_data_format='channels_first', sinusoidal = False, trainable = True, pers_act = 'elu', attention_type='cbam_block',subsample_initial_block = True,strides=(2,2) , \n",
        "                 filters = 64, initial_kernel = (7, 7),debug=False):\n",
        "        \n",
        "        inputs = Input(shape=input_shape) #batch*x*y*3\n",
        "        self.history = None\n",
        "\n",
        "        #Layer 1\n",
        "        '''\n",
        "        if trainable:\n",
        "          fft_base_2 = pow(2, math.ceil(math.log(n_fft)/math.log(2)));\n",
        "\n",
        "          x = Melspectrogram(sr=sample_rate, n_mels=n_mels,n_dft=fft_base_2,  n_hop=hop_length, input_shape=input_shape,return_decibel_melgram=True, trainable_kernel=False,  trainable_fb=True) (inputs)\n",
        "          #x = Spectrogram( n_dft=fft_base_2, n_hop=hop_length, input_shape=(input_shape), win_length=win_length, return_decibel_spectrogram=True, power_spectrogram=2.0, trainable_kernel=True, name='static_stft') (inputs)\n",
        "          #x = Normalization2D(str_axis='freq')(x)\n",
        "        else:\n",
        "        '''\n",
        "        x = self.get_melspectrogram_layer(name='mel',n_fft=n_fft,sample_rate=sample_rate,n_mels=n_mels, hop_length=hop_length, return_decibel=return_decibel,input_data_format=input_data_format,  trainable = trainable, num_classes=outputclasses) (inputs)      \n",
        "        \n",
        "        if debug:\n",
        "          x = Conv2D(128,(1,1),  name='conv2dfirst')(x)\n",
        "        #x2 = get_log_frequency_spectrogram_layer(n_fft=2048,log_n_bins=84, sample_rate=16000,hop_length=344,input_data_format='channels_first',return_decibel=True)(inputs)\n",
        "\n",
        "        #x = keras.layers.Concatenate(axis=2)([x1,x2])#,x4])\n",
        "       \n",
        "        regularizer = keras.regularizers.l2(weight_decay)\n",
        "        self.datasetname = datasetname\n",
        "        self.outputclasses=outputclasses\n",
        "        #x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "        \n",
        "        if sinusoidal == False:\n",
        "            activ = pers_act\n",
        "        else:\n",
        "            print('ERROR --> Not implemented Sine Attention')\n",
        "            activ = 'relu'\n",
        "\n",
        "\n",
        "        if subsample_initial_block:\n",
        "          \n",
        "          initial_strides = strides\n",
        "        else:\n",
        "          \n",
        "          initial_strides = (1, 1)\n",
        "\n",
        "        x = Conv2D(filters, initial_kernel, kernel_initializer='he_normal', padding='same',\n",
        "               strides=initial_strides, use_bias=False, kernel_regularizer=regularizer)(x)\n",
        "        '''\n",
        "        if subsample_initial_block:\n",
        "          x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "          if sinusoidal == False:\n",
        "            x = Activation(pers_act)(x)\n",
        "          else:\n",
        "            x =Dentamaro(trainable=True)(x)\n",
        "          x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "        '''\n",
        "        x = attach_attention_module(x,'cbam_block',activation=activ, ratio = 2)\n",
        "        #END LAYER 1\n",
        "\n",
        "        #Layer 2\n",
        "        #block1, out batch*(x)*(y)*16\n",
        "\n",
        "        for i in range(0,runs[0]):\n",
        "          #if i == 0:\n",
        "          #   identity = Conv2D(filters,(2,2), padding='same', kernel_regularizer=regularizer, name='id1')(x)\n",
        "          x = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=regularizer)(x)\n",
        "          x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "          if sinusoidal == False:\n",
        "            x = Activation(pers_act)(x)\n",
        "          else:\n",
        "            x =Dentamaro(trainable=True)(x)\n",
        "          #x = attach_attention_module(x,attention_type)\n",
        "          x = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=regularizer)(x) #batch*x*y*16\n",
        "          x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "          if sinusoidal == False:\n",
        "            x = Activation(pers_act)(x)\n",
        "          else:\n",
        "            x =Dentamaro(trainable=True)(x)\n",
        "          #x = Add()([identity,x])\n",
        "        x = attach_attention_module(x,attention_type,activation=activ)\n",
        "\n",
        "        #x = Add()([identity,x])\n",
        "        #END Layer 2\n",
        "        \n",
        "        #Layer 3\n",
        "        #block2, out batch*(x/2)*(y/2)*64\n",
        "        for i in range(0,runs[1]):#was 18\n",
        "            identity = x\n",
        "            if i == 0:\n",
        "                identity=Conv2D(filters,(2,2), padding='same', kernel_regularizer=regularizer, name='block2dimchangeconv')(identity)\n",
        "            x = Conv2D(int(filters/4), (1, 1), padding='same', kernel_regularizer=regularizer, name='block2resblock'+str((i+1))+'conv1')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(int(filters/4), (3, 3), padding='same', kernel_regularizer=regularizer, name='block2resblock'+str((i+1))+'conv2')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=regularizer, name='block2resblock'+str((i+1))+'conv3')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            x = attach_attention_module(x,attention_type,activation=activ)\n",
        "            x = Add()([identity,x])\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "        \n",
        "        l1 = x #16 filters, 32x32 resolution\n",
        "        f1 = filters\n",
        "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2pool')(x)\n",
        "\n",
        "        #END Layer 3\n",
        "        #Layer 4\n",
        "\n",
        "        filters *= 2\n",
        "\n",
        "        #block3, out batch*(x/4)*(y/4)*128\n",
        "        for i in range(0,runs[2]):#was18\n",
        "            identity = x\n",
        "            if i == 0:\n",
        "                identity=Conv2D(filters, (2,2), padding='same', kernel_regularizer=regularizer, name='block3dimchangeconv')(identity)            \n",
        "            x = Conv2D(int(filters/4), (1, 1), padding='same', kernel_regularizer=regularizer, name='block3resblock'+str((i+1))+'conv1')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(int(filters/4), (1, 1), padding='same', kernel_regularizer=regularizer, name='block3resblock'+str((i+1))+'conv2')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=regularizer, name='block3resblock'+str((i+1))+'conv3')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            x = attach_attention_module(x,attention_type,activation=activ)\n",
        "            x = Add()([identity,x])\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "        f2 = filters\n",
        "        l2 = x #256 filters, 16x16 resolution\n",
        "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block3pool')(x)\n",
        "        #END Layer 4\n",
        "\n",
        "        #Layer 5\n",
        "        filters *= 2\n",
        "        #block4, out batch*(x/4)*(y/4)*256\n",
        "        for i in range(0,runs[3]):#was18\n",
        "            identity = x\n",
        "            if i == 0:\n",
        "                identity=Conv2D(filters, (2,2), padding='same', kernel_regularizer=regularizer, name='block4dimchangeconv')(identity)            \n",
        "            x = Conv2D(int(filters/4), (1, 1), padding='same', kernel_regularizer=regularizer, name='block4resblock'+str((i+1))+'conv1')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(int(filters/4), (3, 3), padding='same', kernel_regularizer=regularizer, name='block4resblock'+str((i+1))+'conv2')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=regularizer, name='block4resblock'+str((i+1))+'conv3')(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            x = attach_attention_module(x,attention_type,activation=activ)\n",
        "            x = Add()([identity,x])\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "        l3 = x #512 filters, 8x8 resolution\n",
        "        f3 = filters\n",
        "        x = MaxPooling2D((2, 2), strides=(2, 2), name='block4pool')(x)#aggiunto\n",
        "        #END Layer 5\n",
        "        #Layer 6\n",
        "\n",
        "        filters *= 2\n",
        "        #block4, out batch*(x/4)*(y/4)*256\n",
        "        for i in range(0,runs[4]):#was18\n",
        "            identity = x\n",
        "            if i == 0:\n",
        "                identity=Conv2D(filters, (2,2), padding='same', kernel_regularizer=regularizer)(identity)            \n",
        "            x = Conv2D(int(filters/4), (1, 1), padding='same', kernel_regularizer=regularizer)(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(int(filters/4), (3, 3), padding='same', kernel_regularizer=regularizer)(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x)\n",
        "            x = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=regularizer)(x)\n",
        "            x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "            x = attach_attention_module(x,attention_type,activation=activ)\n",
        "            x = Add()([identity,x])\n",
        "            #x = Activation(pers_act)(x)\n",
        "            if sinusoidal == False:\n",
        "              x = Activation(pers_act)(x)\n",
        "            else:\n",
        "              x =Dentamaro(trainable=True)(x) \n",
        "        l4 = x  \n",
        "        f4 = filters\n",
        "        \n",
        "        x = Conv2D(filters, (3, 3), padding='same', kernel_regularizer=regularizer, name='outconv')(x) \n",
        "        x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "        x = MaxPooling2D((2,2), strides=(2,2), name=\"gpool\")(x)\n",
        "        x = attach_attention_module(x,'cbam_block',activation=activ, ratio = 2)\n",
        "\n",
        "        gbase = Flatten(name='pregflatten')(x)\n",
        "        if True:\n",
        "          g64 = SinusodialRepresentationDense(f1, activation='sine', w0=1.0)(gbase)#added now 0.406\n",
        "          g128 = SinusodialRepresentationDense(f2, activation='sine', w0=1.0)(gbase)#added now \n",
        "          g256 = SinusodialRepresentationDense(f3, activation='sine', w0=1.0)(gbase)#added now \n",
        "          g4 = SinusodialRepresentationDense(f4, activation='sine', w0=1.0)(gbase)#added now \n",
        "        else:\n",
        "          g64 = Dense(f1, kernel_regularizer=regularizer, name='globalg64',activation=pers_act)(gbase)\n",
        "          g128 = Dense(f2, kernel_regularizer=regularizer, name='globalg128',activation=pers_act)(gbase)\n",
        "          g256 = Dense(f3, kernel_regularizer=regularizer, name='globalg256',activation=pers_act)(gbase)    \n",
        "          g4 = Dense(f4, kernel_regularizer=regularizer, name='globalg4',activation=pers_act)(gbase)       \n",
        "\n",
        "        #Learnable attention\n",
        "        c1 = ParametrisedCompatibility(kernel_regularizer=regularizer, name='cpc1')([l1, g64])  # batch*x*y\n",
        "        if compatibilityfunction == 'mha':\n",
        "            c1 =  tfa.layers.MultiHeadAttention(head_size=64, num_heads=12)([l1, g64])\n",
        "        if compatibilityfunction == 'dp':\n",
        "            c1 = Lambda(lambda lam: K.squeeze(K.map_fn(lambda xy: K.dot(xy[0], xy[1]), elems=(lam[0], K.expand_dims(lam[1], -1)), dtype='float32'), 3), name='cdp1')([l1, g64])  # batch*x*y\n",
        "        flatc1 = Flatten(name='flatc1')(c1)  # batch*xy\n",
        "        a1 = Activation('softmax', name='softmax1')(flatc1)  # batch*xy\n",
        "        reshaped1 = Reshape((-1,f1), name='reshape1')(l1)  # batch*xy*256.\n",
        "        if compatibilityfunction == 'mha':\n",
        "            g1 = a1\n",
        "        else:\n",
        "            g1 = Lambda(lambda lam: K.squeeze(K.batch_dot(K.expand_dims(lam[0], 1), lam[1]), 1), name='g1')([a1, reshaped1])  # batch*256.\n",
        "        \n",
        "        c2 = ParametrisedCompatibility(kernel_regularizer=regularizer, name='cpc2')([l2, g128])\n",
        "        if compatibilityfunction == 'mha':\n",
        "            c2 =  tfa.layers.MultiHeadAttention(head_size=64, num_heads=12)([l2, g128])\n",
        "        if compatibilityfunction == 'dp':\n",
        "            c2 = Lambda(lambda lam: K.squeeze(K.map_fn(lambda xy: K.dot(xy[0], xy[1]), elems=(lam[0], K.expand_dims(lam[1], -1)), dtype='float32'), 3), name='cdp2')([l2, g128])\n",
        "        flatc2 = Flatten(name='flatc2')(c2)\n",
        "        a2 = Activation('softmax', name='softmax2')(flatc2)\n",
        "        reshaped2 =  Reshape((-1,f2), name='reshape2')(l2)\n",
        "        if compatibilityfunction == 'mha':\n",
        "            g2 = a2\n",
        "        else:\n",
        "            g2 = Lambda(lambda lam: K.squeeze(K.batch_dot(K.expand_dims(lam[0], 1), lam[1]), 1), name='g2')([a2, reshaped2])\n",
        "\n",
        "        c3 = ParametrisedCompatibility(kernel_regularizer=regularizer, name='cpc3')([l3, g256])\n",
        "        if compatibilityfunction == 'mha':\n",
        "            c3 =  tfa.layers.MultiHeadAttention(head_size=64, num_heads=12)([l3, g256])\n",
        "        if compatibilityfunction == 'dp':\n",
        "            c3 = Lambda(lambda lam: K.squeeze(K.map_fn(lambda xy: K.dot(xy[0], xy[1]), elems=(lam[0], K.expand_dims(lam[1], -1)), dtype='float32'), 3), name='cdp3')([l3, g256])\n",
        "        flatc3 = Flatten(name='flatc3')(c3)\n",
        "        a3 = Activation('softmax', name='softmax3')(flatc3)\n",
        "        reshaped3 = Reshape((-1,f3), name='reshape3')(l3)\n",
        "        if compatibilityfunction == 'mha':\n",
        "            g3 = a3\n",
        "        else:\n",
        "            g3 = Lambda(lambda lam: K.squeeze(K.batch_dot(K.expand_dims(lam[0], 1), lam[1]), 1), name='g3')([a3, reshaped3])\n",
        "        \n",
        "        if runs[4] > 0:\n",
        "          c4 = ParametrisedCompatibility(kernel_regularizer=regularizer, name='cpc4')([l4, g4])\n",
        "          if compatibilityfunction == 'mha':\n",
        "              c4 =  tfa.layers.MultiHeadAttention(head_size=64, num_heads=12)([l4, g4])\n",
        "          if compatibilityfunction == 'dp':\n",
        "              c4 = Lambda(lambda lam: K.squeeze(K.map_fn(lambda xy: K.dot(xy[0], xy[1]), elems=(lam[0], K.expand_dims(lam[1], -1)), dtype='float32'), 3), name='cdp4')([l4, g4])\n",
        "          flatc4 = Flatten(name='flatc4')(c4)\n",
        "          a4 = Activation('softmax', name='softmax4')(flatc4)\n",
        "          reshaped4 = Reshape((-1,f4), name='reshape4')(l4)\n",
        "          if compatibilityfunction == 'mha':\n",
        "              g4_ = a4\n",
        "          else:\n",
        "              g4_ = Lambda(lambda lam: K.squeeze(K.batch_dot(K.expand_dims(lam[0], 1), lam[1]), 1), name='g4')([a4, reshaped4])\n",
        "\n",
        "        out = ''\n",
        "        if gmode == 'concat':\n",
        "            if runs[4] > 0:\n",
        "              glist = [g4_, g3, g2, g1]\n",
        "            else:\n",
        "              glist = [g3, g2, g1]\n",
        "            predictedG = Concatenate(axis=1, name='ConcatG')(glist)\n",
        "            x = Dense(outputclasses, kernel_regularizer=regularizer, name=str(outputclasses)+'ConcatG')(predictedG)\n",
        "            out = Activation(\"softmax\", name='concatsoftmaxout')(x)\n",
        "            \n",
        "        else:\n",
        "            gd3 = Dense(outputclasses, activation='softmax', name=str(outputclasses)+'indepsoftmaxg3')(g3)\n",
        "            gd4 = Dense(outputclasses, activation='softmax', name=str(outputclasses)+'indepsoftmaxg4')(g4_)\n",
        "            gd2 = Dense(outputclasses, activation='softmax', kernel_regularizer=regularizer, name=str(outputclasses)+'indepsoftmaxg2')(g2)\n",
        "            gd1 = Dense(outputclasses, activation='softmax', kernel_regularizer=regularizer, name=str(outputclasses)+'indepsoftmaxg1')(g1)\n",
        "            if runs[4] > 0:\n",
        "              out = Add(name='addg4g3g2g1')([gd1, gd2, gd3, gd4])\n",
        "              out = Lambda(lambda lam: lam/4, name='4average')(out)\n",
        "            else:\n",
        "              out = Add(name='addg4g3g2g1')([gd1, gd2, gd3])\n",
        "              out = Lambda(lambda lam: lam/3, name='4average')(out)\n",
        "\n",
        "        #END Layer 6\n",
        "        model = Model(inputs=inputs, outputs=out)\n",
        "        \n",
        "      \n",
        "\n",
        "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)#, run_eagerly=True)\n",
        "        tf.keras.utils.plot_model(\n",
        "            model, to_file='model.png', show_shapes=True, show_dtype=True,\n",
        "            show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        "        )\n",
        "        name = (\"(RN-\"+att+\")-\"+gmode+\"-\"+compatibilityfunction).replace('att)', 'att1)')\n",
        "        print(\"Generated \"+name)\n",
        "        self.name = name\n",
        "        self.model = model\n",
        "    \n",
        "    def StandardFit(self, datasetname=None, X=[], Y=[],initial_lr=0.01, min_delta=None, patience=3, datagen = None, validation_data=None, lrplateaufactor=None, lrplateaupatience=4, validation_split=0.3,batch_size=16,epochs=100,checkpointcall=None, class_weights=None, monitor_early='val_auc'):\n",
        "        #Y = keras.utils.to_categorical(Y,self.outputclasses)\n",
        "        if datasetname==None:\n",
        "            datasetname=self.datasetname\n",
        "        if os.path.isfile(\"weights/\"+self.name+\"-\"+datasetname+\" early.hdf5\"):\n",
        "            print(\"Found early-stopped weights for \"+self.name+\"-\"+datasetname)\n",
        "            return\n",
        "        scheduler = LearningRateScaler([20, 50, 80], 0.2, initial_lr)\n",
        "        \n",
        "        tboardcb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=3, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
        "        if checkpointcall ==None:\n",
        "            checkpoint = ModelCheckpoint(\"weights/\"+self.name+\"-\"+datasetname+\" {epoch}.hdf5\", \n",
        "                                         save_weights_only=True,monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "        else:\n",
        "            checkpoint = checkpointcall\n",
        "        epochprint = LambdaCallback(on_epoch_end=lambda epoch, logs: print(\"Passed epoch \"+str(epoch)))\n",
        "        \n",
        "        callbackslist = [ checkpoint, epochprint]#, tboardcb]\n",
        " \n",
        "         \n",
        "        if min_delta != None:\n",
        "              callbackslist.append(EarlyStopping(monitor=monitor_early, min_delta=min_delta, patience=patience))\n",
        "        if lrplateaufactor != None:\n",
        "              callbackslist.append(ReduceLROnPlateau(monitor='auc', factor = lrplateaufactor, patience = lrplateaupatience))\n",
        "        if lrplateaufactor != None:\n",
        "              callbackslist.append(ReduceLROnPlateau(monitor='accuracy', factor = lrplateaufactor, patience = lrplateaupatience))\n",
        "        if datagen != None:\n",
        "          self.history = self.model.fit(datagen, epochs=epochs, callbacks=callbackslist, shuffle=False,\n",
        "                               validation_data=(validation_data[0],validation_data[1]), verbose=1)\n",
        "        else:  \n",
        "          if validation_data == None:\n",
        "              self.history = self.model.fit(X, Y,  batch_size=batch_size, epochs=epochs, \n",
        "                                            callbacks=callbackslist, shuffle=False,validation_split=validation_split, verbose=1, class_weight=class_weights)#,class_weight=class_weights)    \n",
        "          else:\n",
        "              self.history = self.model.fit(X, Y,  batch_size=batch_size, epochs=epochs, callbacks=callbackslist, shuffle=False,\n",
        "                            validation_data=(validation_data[0],validation_data[1]), verbose=1, class_weight=class_weights)#,class_weight=class_weights)     \n",
        "        \n",
        "        \n",
        "        return self.model\n",
        "        \n",
        "        \n",
        "        return self.model\n",
        "    \n",
        "    def save_plot(self, filename, history=None, title='AUCO ResNet Accuracy'):\n",
        "        \n",
        "        if history == None:\n",
        "            history = self.history\n",
        "       \n",
        "        plt.rcParams[\"figure.figsize\"] = [16,9]\n",
        "            \n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        #plt.plot(history.history['loss'])\n",
        "        #plt.plot(history.history['val_loss'])\n",
        "        plt.plot(history.history['auc'])\n",
        "        plt.plot(history.history['val_auc'])\n",
        "            \n",
        "\n",
        "        plt.title(title)\n",
        "        plt.ylabel('Accuracy / AUC')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['training accuracy', 'validation accuracy','training AUC','validation AUC'], loc='upper left')\n",
        "        plt.savefig(filename, format=\"svg\", cmap='nipy_spectral')\n",
        "        f = plt.figure()\n",
        "        f.clear()\n",
        "        plt.close(f)\n",
        "        plt.close('all')\n",
        "\n",
        " \n",
        "    def get_melspectrogram_layer(self,\n",
        "        input_shape=None,\n",
        "        n_fft=2048,\n",
        "        win_length=None,\n",
        "        hop_length=None,\n",
        "        window_name=None,\n",
        "        pad_begin=False,\n",
        "        pad_end=False,\n",
        "        sample_rate=22050,\n",
        "        n_mels=128,\n",
        "        mel_f_min=0.0,\n",
        "        mel_f_max=None,\n",
        "        mel_htk=False,\n",
        "        mel_norm='slaney',\n",
        "        return_decibel=False,\n",
        "        db_amin=1e-5,\n",
        "        db_ref_value=1.0,\n",
        "        db_dynamic_range=80.0,\n",
        "        input_data_format='default',\n",
        "        output_data_format='default',\n",
        "        trainable = True,\n",
        "        name='melspectrogram',\n",
        "        num_classes=2,\n",
        "    ):\n",
        "        \"\"\"A function that returns a melspectrogram layer, which is a `keras.Sequential` model consists of\n",
        "        `STFT`, `Magnitude`, `ApplyFilterbank(_mel_filterbank)`, and optionally `MagnitudeToDecibel`.\n",
        "        Args:\n",
        "            input_shape (None or tuple of integers): input shape of the model. Necessary only if this melspectrogram layer is\n",
        "                is the first layer of your model (see `keras.model.Sequential()` for more details)\n",
        "            n_fft (int): number of FFT points in `STFT`\n",
        "            win_length (int): window length of `STFT`\n",
        "            hop_length (int): hop length of `STFT`\n",
        "            window_name (str or None): *Name* of `tf.signal` function that returns a 1D tensor window that is used in analysis.\n",
        "                Defaults to `hann_window` which uses `tf.signal.hann_window`.\n",
        "                Window availability depends on Tensorflow version. More details are at `kapre.backend.get_window()`.\n",
        "            pad_begin (bool): Whether to pad with zeros along time axis (length: win_length - hop_length). Defaults to `False`.\n",
        "            pad_end (bool): whether to pad the input signal at the end in `STFT`.\n",
        "            sample_rate (int): sample rate of the input audio\n",
        "            n_mels (int): number of mel bins in the mel filterbank\n",
        "            mel_f_min (float): lowest frequency of the mel filterbank\n",
        "            mel_f_max (float): highest frequency of the mel filterbank\n",
        "            mel_htk (bool): whether to follow the htk mel filterbank fomula or not\n",
        "            mel_norm ('slaney' or int): normalization policy of the mel filterbank triangles\n",
        "            return_decibel (bool): whether to apply decibel scaling at the end\n",
        "            db_amin (float): noise floor of decibel scaling input. See `MagnitudeToDecibel` for more details.\n",
        "            db_ref_value (float): reference value of decibel scaling. See `MagnitudeToDecibel` for more details.\n",
        "            db_dynamic_range (float): dynamic range of the decibel scaling result.\n",
        "            input_data_format (str): the audio data format of input waveform batch.\n",
        "                `'channels_last'` if it's `(batch, time, channels)`\n",
        "                `'channels_first'` if it's `(batch, channels, time)`\n",
        "                Defaults to the setting of your Keras configuration. (tf.keras.backend.image_data_format())\n",
        "            output_data_format (str): the data format of output melspectrogram.\n",
        "                `'channels_last'` if you want `(batch, time, frequency, channels)`\n",
        "                `'channels_first'` if you want `(batch, channels, time, frequency)`\n",
        "                Defaults to the setting of your Keras configuration. (tf.keras.backend.image_data_format())\n",
        "            name (str): name of the returned layer\n",
        "        Note:\n",
        "            Melspectrogram is originally developed for speech applications and has been *very* widely used for audio signal\n",
        "            analysis including music information retrieval. As its mel-axis is a non-linear compression of (linear)\n",
        "            frequency axis, a melspectrogram can be an efficient choice as an input of a machine learning model.\n",
        "            We recommend to set `return_decibel=True`.\n",
        "            **References**:\n",
        "            `Automatic tagging using deep convolutional neural networks <https://arxiv.org/abs/1606.00298>`_,\n",
        "            `Deep content-based music recommendation <http://papers.nips.cc/paper/5004-deep-content-based-music-recommen>`_,\n",
        "            `CNN Architectures for Large-Scale Audio Classification <https://arxiv.org/abs/1609.09430>`_,\n",
        "            `Multi-label vs. combined single-label sound event detection with deep neural networks <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.711.74&rep=rep1&type=pdf>`_,\n",
        "            `Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification <https://arxiv.org/pdf/1608.04363.pdf>`_,\n",
        "            and way too many speech applications.\n",
        "        Example:\n",
        "            ::\n",
        "                input_shape = (2, 2048)  # stereo signal, audio is channels_first\n",
        "                melgram = get_melspectrogram_layer(input_shape=input_shape, n_fft=1024, return_decibel=True,\n",
        "                    n_mels=96, input_data_format='channels_first', output_data_format='channels_last')\n",
        "                model = Sequential()\n",
        "                model.add(melgram)\n",
        "                # now the shape is (batch, n_frame=3, n_mels=96, n_ch=2) because output_data_format is 'channels_last'\n",
        "                # and the dtype is float\n",
        "        \"\"\"\n",
        "        kapre.backend.validate_data_format_str(input_data_format)\n",
        "        kapre.backend.validate_data_format_str(output_data_format)\n",
        "\n",
        "        stft_kwargs = {}\n",
        "        if input_shape is not None:\n",
        "            stft_kwargs['input_shape'] = input_shape\n",
        "\n",
        "        waveform_to_stft = STFT(\n",
        "            **stft_kwargs,\n",
        "            n_fft=n_fft,\n",
        "            win_length=win_length,\n",
        "            hop_length=hop_length,\n",
        "            window_name=window_name,\n",
        "            pad_begin=pad_begin,\n",
        "            pad_end=pad_end,\n",
        "            input_data_format=input_data_format,\n",
        "            output_data_format=output_data_format,\n",
        "        )\n",
        "\n",
        "        stft_to_stftm = Magnitude()\n",
        "\n",
        "        kwargs = {\n",
        "            'sample_rate': sample_rate,\n",
        "            'n_freq': n_fft // 2 + 1,\n",
        "            'n_mels': n_mels,\n",
        "            'f_min': mel_f_min,\n",
        "            'f_max': mel_f_max,\n",
        "            'htk': mel_htk,\n",
        "            'trainable': trainable,\n",
        "            'norm': mel_norm,\n",
        "            'num_classes':num_classes,\n",
        "        }\n",
        "        stftm_to_melgram = ApplyFilterbank(\n",
        "            type='mel', filterbank_kwargs=kwargs, data_format=output_data_format\n",
        "        )\n",
        "\n",
        "        \n",
        "        mag_to_decibel = MagnitudeToDecibel(\n",
        "                ref_value=db_ref_value, amin=db_amin, dynamic_range=db_dynamic_range\n",
        "            )\n",
        " \n",
        "         \n",
        "        layers = [waveform_to_stft, stft_to_stftm, stftm_to_melgram, mag_to_decibel] \n",
        "        return Sequential(layers, name=name)\n",
        "        \n",
        "\n",
        "class TrainableMel(Layer):\n",
        "\n",
        "    def __init__(self, kernel_regularizer=None, **kwargs):\n",
        "        super(TrainableMel, self).__init__(**kwargs)\n",
        "        if kernel_regularizer == None:\n",
        "          self.regularizer = keras.regularizers.l2(0.0005)#era 0.0005\n",
        "        else:\n",
        "          self.regularizer = kernel_regularizer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.kernel = self.add_weight(name='kernel',  initializer=tf.keras.initializers.GlorotUniform(), \n",
        "                                      regularizer = self.regularizer, trainable=True,\n",
        "                                      #shape=(input_shape[-1], input_shape[-2]))\n",
        "                                      shape=(input_shape[1], input_shape[2], input_shape[3]))\n",
        "        super(TrainableMel, self).build(input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        \n",
        "        return config\n",
        "\n",
        "    def call(self, x):  \n",
        "        \n",
        "        #Hadamard product\n",
        "        #K.print_tensor(x, message='x = ')\n",
        "        #return x * self.kernel\n",
        "        return x*self.kernel\n",
        "        #return tf.keras.backend.dot(x,self.kernel)\n",
        "\n",
        "\n",
        "class ParametrisedCompatibility(Layer):\n",
        "\n",
        "    def __init__(self, kernel_regularizer=None, **kwargs):\n",
        "        super(ParametrisedCompatibility, self).__init__(**kwargs)\n",
        "        self.regularizer = kernel_regularizer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.u = self.add_weight(name='u', shape=(input_shape[0][3], 1), initializer='uniform',\n",
        "                                 regularizer=self.regularizer, trainable=True)\n",
        "        super(ParametrisedCompatibility, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):  # add l and g. Dot the sum with u.\n",
        "        return K.dot(K.map_fn(lambda lam: (lam[0]+lam[1]),elems=(x),dtype='float32'), self.u)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][1], input_shape[0][2])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        \n",
        "        return config\n",
        "\n",
        "\n",
        "\n",
        "class Dentamaro(Layer):\n",
        "\n",
        "    def __init__(self, alpha=1.0,  trainable=True, **kwargs):\n",
        "        super(Dentamaro, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.alpha = alpha\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.alpha_factor = K.variable(self.alpha,\n",
        "                                      dtype=K.floatx(),\n",
        "                                      name='alpha_factor')\n",
        "        if self.trainable:\n",
        "            self._trainable_weights.append(self.alpha_factor)\n",
        "\n",
        "        super(Dentamaro, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        x = inputs\n",
        "        #x +1 - (cos( 4x)-x/1.5) /(e^(-x/4)) \n",
        "        #t = x-tf.exp(x/self.alpha_factor) * (tf.cos(self.alpha_factor*x)-self.beta_factor*x) +1\n",
        "        #da provare ( sin(5x)/3 -1)+  e^x\n",
        "        #t = (tf.sin(self.alpha_factor*x)/self.beta_factor) -1 + x#tf.math.abs(x)#tf.math.log(1+tf.math.abs(x))#+ 2**x\n",
        "        #t = x + (1 - tf.sin(self.alpha_factor*x) +x/self.beta_factor) / self.alpha_factor\n",
        "        #t = (tf.sin(self.alpha_factor*x)/self.beta_factor)*tf.math.abs(x)+x\n",
        "        x = tf.convert_to_tensor(x)\n",
        "        return tf.sin(self.alpha_factor*x)#(1-x*x)*tf.exp(-self.alpha_factor*x*x)\n",
        "        #or\n",
        "        #return 2.0*tf.sin(tf.pi*x)*x*tf.exp(-self.alpha_factor*x*x)\n",
        "        #\n",
        "        #return tf.sin(x*self.alpha_factor)#x + (1 - tf.cos(2 * self.alpha_factor * x)) / (2 * self.alpha_factor)\n",
        "        #return t\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'alpha': self.get_weights()[0] if self.trainable else self.alpha,\n",
        "                  'trainable': self.trainable}\n",
        "        base_config = super(Dentamaro, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "class TrainedKLDivergence(Layer):\n",
        "    #Dentamaro et al.\n",
        "    def __init__(self, kernel_regularizer=None, **kwargs):\n",
        "        super(TrainedKLDivergence, self).__init__(**kwargs)\n",
        "        self.regularizer = kernel_regularizer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.u = self.add_weight(name='u', shape=(input_shape[0][3], 1), initializer='uniform', regularizer=self.regularizer, trainable=True)\n",
        "        super(TrainedKLDivergence, self).build(input_shape)\n",
        "\n",
        "    '''\n",
        "    ## normalize p, q to probabilities\n",
        "    p, q = p/p.sum(), q/q.sum()\n",
        "    m = 1./2*(p + q)\n",
        "    return sp.stats.entropy(p,m, base=base)/2. +  sp.stats.entropy(q, m, base=base)/2.\n",
        "    '''\n",
        "\n",
        "    def call(self, x):\n",
        "        # Kullback-Leibler divergence \n",
        "\n",
        "        #p = x[0] / K.sum(x[0])\n",
        "        #q = x[1] / K.sum(x[1])\n",
        "        \n",
        "        mapping = K.map_fn(lambda lam: (K.pow(2.718,(lam[0]+lam[1]))),elems=(x),dtype='float32')\n",
        "        \n",
        "        #Dot the sum with u.\n",
        "        return K.dot(mapping, self.u)\n",
        "        #return K.dot(K.map_fn(lambda lam: (lam[0]+lam[1]),elems=(x),dtype='float32'), self.u)\n",
        "\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][1], input_shape[0][2])\n",
        "    def logbase(self, x,base):\n",
        "        numerator = K.log(x)\n",
        "        denominator = K.log(base)\n",
        "        return numerator / denominator\n",
        "\n",
        "       \n",
        "class LearningRateScaler(Callback):\n",
        "    \n",
        "    def __init__(self, epochs, multiplier, initial_lr=None):\n",
        "        self.multiplier = multiplier\n",
        "        self.epochs = epochs\n",
        "        self.initial_lr = initial_lr\n",
        "        self.startingepoch = True\n",
        "    \n",
        "    def on_train_begin(self, logs=None):\n",
        "        if self.initial_lr == None:\n",
        "            self.initial_lr = K.get_value(self.model.optimizer.lr)\n",
        "        print(\"Initial lr=\"+str(self.initial_lr))\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        #print(\"Current lr: \" + str(K.get_value(self.model.optimizer.lr)))\n",
        "        lr = self.initial_lr\n",
        "        #print('epochs')\n",
        "        #print(self.epochs)\n",
        "        if epoch>0 and epoch in self.epochs:\n",
        "                for i in range(0, self.epochs.index(epoch)+1):\n",
        "                    lr = lr * self.multiplier\n",
        "                K.set_value(self.model.optimizer.lr, lr)\n",
        "                #print(\"Updated learning rate to \"+str(lr))    \n",
        "        \n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        startingepoch = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwUtVrpeOi4c",
        "outputId": "049f126d-6fba-4c37-f3ec-d15b368c1aea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqx2XAv_ZIpU"
      },
      "source": [
        "\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "aa = []\n",
        "saucs = []\n",
        "saa = []\n",
        "histories = []\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-kAqFfkT0d_",
        "outputId": "7bd54390-33e4-471c-d664-8240ace90872"
      },
      "source": [
        "from sklearn.metrics import  confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc \n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error \n",
        "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error\n",
        "#from sklearn.cross_validation import KFold, cross_val_score\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import copy\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        " \n",
        "\n",
        "def concilie_per_patient_res(predx, y_test, usercodes):\n",
        "    new_pred = []\n",
        "    new_y_test = []\n",
        "     \n",
        "    #print('Prediced shape')\n",
        "    #print(predx.shape)\n",
        "    \n",
        "    if y_test.shape[-1] < 2:\n",
        "      y_test = to_categorical(y_test)\n",
        "    patient = {}\n",
        "    for i in range(len(y_test)):\n",
        "      if not usercodes[i] in patient:\n",
        "        patient[usercodes[i]] = {}\n",
        "        patient[usercodes[i]]['predicted'] = []\n",
        "        patient[usercodes[i]]['y_test'] = []               \n",
        "      patient[usercodes[i]]['predicted'].append(predx[i])\n",
        "      patient[usercodes[i]]['y_test'].append(y_test[i])\n",
        "      #print(patient[usercodes[i]]['predicted'])\n",
        "    keys = list(patient.keys())\n",
        "    for key in keys:\n",
        "       predi = patient[key]['predicted']\n",
        "       yi = patient[key]['y_test']\n",
        "       mean_pred = np.asarray(predi).mean(axis=0)\n",
        "       mean_y = np.asarray(yi).mean(axis=0)\n",
        "       #print('Mean pred shape '+str(mean_pred.shape))\n",
        "       #print('Mean y shape '+str(mean_y.shape))\n",
        "       new_pred.append(mean_pred)\n",
        "       new_y_test.append(mean_y)\n",
        "\n",
        "    new_pred = np.asarray(new_pred)\n",
        "    #print(new_pred.shape)\n",
        "    new_y_test = np.asarray(new_y_test)\n",
        "    #print(new_y_test.shape)\n",
        "\n",
        "    return new_pred, new_y_test\n",
        "    \n",
        " \n",
        "\n",
        "\n",
        "def report_average(reports):\n",
        "    accuracy = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "    for report in reports:\n",
        "        print(report)\n",
        "        accuracy.append(report['accuracy'])\n",
        "        precision.append(report['weighted avg']['precision'])\n",
        "        recall.append(report['weighted avg']['recall'])\n",
        "        f1.append(report['weighted avg']['f1-score'])\n",
        "\n",
        "    print('Mean accuracy '+str(np.mean(accuracy)))\n",
        "    print('Mean precision ' + str(np.mean(precision)))\n",
        "    print('Mean recall ' + str(np.mean(recall)))\n",
        "    print('Mean F1 ' + str(np.mean(f1)))\n",
        "\n",
        "\n",
        "\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "def custom_act_dentamaro(x):#inserire formula qui\n",
        "    #x +1 - (cos( 4x)-x/1.5) /(e^(-x/4)) \n",
        "   x = tf.convert_to_tensor(x)\n",
        "   return x + (1 - tf.cos(2*x) +x/5) / 2\n",
        "    #return x + (1 - tf.cos(2 * frequency * x)) / (2 * frequency)\n",
        "\n",
        "import tensorflow as tf \n",
        "\n",
        "if tf.test.gpu_device_name(): \n",
        "\n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "else:\n",
        "\n",
        "   print(\"Please install GPU version of TF\")\n",
        "          \n",
        "tf.random.set_seed(42)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "    #K.clear_session()#puliamo la ram della GPU \n",
        "    #tf.keras.backend.clear_session()\n",
        " \n",
        "    y_new = copy.deepcopy(np.asarray(y,dtype=np.float32))\n",
        "    X_new = copy.deepcopy(np.asarray(X,dtype=np.float32))\n",
        "    y_size = 0\n",
        "\n",
        "    X_train, X_test, y_train, y_test, train_usercodes, test_usercodes = inter_patient_scheme(X_new, y_new, patientids, test_size=0.2)\n",
        "   \n",
        "    input_shape = (1, X_train[0].shape[1])\n",
        "     \n",
        "    fft = 1792#1792#128 #arriva a 2048\n",
        "    optimizer = 'RMSprop'\n",
        "    batch_size = 8\n",
        "    \n",
        "\n",
        "    subsample = True\n",
        "    test_AUCO = True\n",
        "    for k in range(1):\n",
        "      model_checkpoint_callback = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "                    filepath='best_model_aucoresnet_covid_iteration_'+str(i)+'.h5',\n",
        "                    save_weights_only=True,\n",
        "                    monitor='val_auc',\n",
        "                    mode='max',\n",
        "                    save_best_only=True)\n",
        "    \n",
        "      sinusoidal = False\n",
        "      pers_act = 'elu'\n",
        "      attention_type='se_block'\n",
        "      kernel_size = (5,5)\n",
        "      batch_size = 16\n",
        "      if k == 1:\n",
        "        kernel_size = (7,7)\n",
        "        pers_act = 'relu'\n",
        "        sinusoidal = False \n",
        "        \n",
        "          \n",
        "      if k == 2:\n",
        "        kernel_size = (13,13)\n",
        "        pers_act = 'relu'\n",
        "        sinusoidal = False \n",
        "        #fft = fft  \n",
        "       \n",
        "     \n",
        "        \n",
        "        #fft =  fft + 128\n",
        "      #print('FFT is '+str(fft))\n",
        "      print('ACtivation ',pers_act)\n",
        "      print('Sinusoida ', sinusoidal)\n",
        "      print('Attention is ', attention_type)\n",
        "      print('Subsample is ', subsample)\n",
        "      \n",
        "      #print('Spiking is '+str(spiking))\n",
        "      model2 = None\n",
        "      if test_AUCO == True:\n",
        "          print('AUCS len', len(aucs))\n",
        "          #Layer 1 =  13% , layer 2 = 11%, Layer 3 17%, Layer 4 10%, Layer 5 26%, Layer 6 22%\n",
        "          \n",
        "          AUCOResNetV2_ =  AUCOResNetV2(att='att3',gmode='concat',sample_rate = SAMPLING_RATE,compatibilityfunction='cp',#cp, mha, dp\n",
        "                                        datasetname='covid',input_shape=input_shape,\n",
        "                                        outputclasses=2,runs=[1,8,3,7,2],n_mels=150, win_length=140, hop_length=344, strides=(3,3),\n",
        "                                        n_fft=fft, sinusoidal = sinusoidal, trainable=True,\n",
        "                                        metrics = ['accuracy', 'AUC'], pers_act = pers_act,attention_type=attention_type, optimizer=optimizer,\n",
        "                                        subsample_initial_block=subsample, initial_kernel=kernel_size, debug=False)#78 auc medio con cbam\n",
        "          \n",
        "          \n",
        "          AUCOResNetV2_.StandardFit(datasetname='covid',X=X_train,Y=y_train,patience=5,validation_data=(X_test,y_test),\n",
        "                                    batch_size=batch_size,epochs=200,checkpointcall = model_checkpoint_callback)\n",
        "          \n",
        "         \n",
        "          model2 = AUCOResNetV2_.model\n",
        "          \n",
        "          histories.append(AUCOResNetV2_.history)\n",
        "          AUCOResNetV2_.save_plot('Iteration_'+str(i)+'_fft_'+str(fft)+'_activation_'+str(pers_act)+'_sinusoidal_'+str(sinusoidal)+'_'+str(optimizer)+'.svg')\n",
        "          \n",
        "      else:\n",
        "          input_shape = (1, X_train[0].shape[1]) \n",
        "          \n",
        "          pre = keras.layers.Input(input_shape)\n",
        "            \n",
        "\n",
        "          pre = AUCOResNetV2().get_melspectrogram_layer(n_fft=fft,sample_rate=SAMPLING_RATE,n_mels=150, win_length=140, hop_length=344, input_data_format='channels_first',trainable = False) (pre)# NICOLA QUI\n",
        "\n",
        "          #pre = AUCOResNetV2().get_log_frequency_spectrogram_layer(n_fft=2048,log_n_bins=84, sample_rate=16000,win_length=160,hop_length=344,input_data_format='channels_first',return_decibel=True, trainable=True)(pre)\n",
        "       \n",
        "\n",
        "          #concatenate both \n",
        "          x = tensorflow.keras.applications.ResNet50(\n",
        "              weights=None, input_tensor=pre,\n",
        "              include_top=False) \n",
        "          x.trainable = True\n",
        "\n",
        "          out = keras.layers.GlobalAveragePooling2D()(x.output)\n",
        "          #out = keras.layers.Dense(32,activation=pers_act)(out)\n",
        "          out = keras.layers.Dense(2,activation='softmax')(out)\n",
        "          model = keras.Model(inputs=x.input, outputs=out)\n",
        "\n",
        "          \n",
        "          model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy','AUC'])\n",
        "\n",
        "          history = model.fit(X_train,y_train, epochs=200, validation_data=(X_test,y_test), batch_size=batch_size,callbacks=[model_checkpoint_callback])\n",
        "          histories.append(history)\n",
        "          AUCOResNetV2().save_plot('Iteration_'+str(i)+'_fft_'+str(fft)+'_activation_'+str(pers_act)+'_DenseNet201_'+str(optimizer)+'.svg', history=history, title='DenseNet 201 Accuracy')\n",
        "          model2 = model\n",
        "        \n",
        "          \n",
        "          \n",
        "      if model2 != None:\n",
        "        model2.load_weights('best_model_aucoresnet_covid_iteration_'+str(i)+'.h5')\n",
        "        y_pred = model2.predict(X_test)\n",
        "\n",
        "        new_pred, new_y_test = concilie_per_patient_res(y_pred, y_test, test_usercodes)\n",
        "         \n",
        "          \n",
        "        print(classification_report(np.argmax(new_y_test, axis=1),np.argmax(new_pred, axis=1)))\n",
        "        if sinusoidal:\n",
        "            saa.append(classification_report(np.argmax(new_y_test, axis=1),np.argmax(new_pred, axis=1),output_dict=True))\n",
        "        else:\n",
        "          aa.append(classification_report(np.argmax(new_y_test, axis=1),np.argmax(new_pred, axis=1),output_dict=True))\n",
        "        roc_auc = roc_auc_score(new_y_test, new_pred, average='weighted' )\n",
        "        print('Roc '+ str(roc_auc))\n",
        "        if sinusoidal:\n",
        "          saucs.append(roc_auc)\n",
        "        else:\n",
        "          aucs.append(roc_auc)\n",
        "        # Plot non-normalized confusion matrix\n",
        "        print('Confusion Matrix')\n",
        "        print(confusion_matrix(np.argmax(new_y_test, axis=1),np.argmax(new_pred, axis=1)))\n",
        "\n",
        "print('Metric '+pers_act)\n",
        "\n",
        "print(report_average(aa))\n",
        "print('AUC')\n",
        "print(np.mean(aucs))\n",
        "\n",
        "print('Metric Sinusoidal')\n",
        "\n",
        "print(report_average(saa))\n",
        "print('AUC')\n",
        "print(np.mean(saucs))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device:/device:GPU:0\n",
            "minority class [0.]\n",
            "minority people 5\n",
            "ACtivation  elu\n",
            "Sinusoida  False\n",
            "Attention is  se_block\n",
            "Subsample is  True\n",
            "AUCS len 6\n",
            "FF shape (897, 150)\n",
            "Trainable mel spectrogram is True\n",
            "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.62367 to fit\n",
            "\n",
            "Generated (RN-att3)-concat-cp\n",
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.5054 - accuracy: 0.5000 - auc: 0.5800Passed epoch 0\n",
            "1/1 [==============================] - 56s 56s/step - loss: 3.5054 - accuracy: 0.5000 - auc: 0.5800 - val_loss: 2.5613 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 57.6014 - accuracy: 0.5000 - auc: 0.5000Passed epoch 1\n",
            "1/1 [==============================] - 1s 762ms/step - loss: 57.6014 - accuracy: 0.5000 - auc: 0.5000 - val_loss: 2.5241 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 15.7365 - accuracy: 0.5000 - auc: 0.5000Passed epoch 2\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 15.7365 - accuracy: 0.5000 - auc: 0.5000 - val_loss: 2.7544 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 5.1589 - accuracy: 0.5000 - auc: 0.6200Passed epoch 3\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 5.1589 - accuracy: 0.5000 - auc: 0.6200 - val_loss: 2.4758 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 9.9220 - accuracy: 0.5000 - auc: 0.5000Passed epoch 4\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 9.9220 - accuracy: 0.5000 - auc: 0.5000 - val_loss: 2.5201 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.0173 - accuracy: 0.9000 - auc: 0.9400Passed epoch 5\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 3.0173 - accuracy: 0.9000 - auc: 0.9400 - val_loss: 2.5201 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.7261 - accuracy: 0.9000 - auc: 0.9500Passed epoch 6\n",
            "1/1 [==============================] - 1s 647ms/step - loss: 2.7261 - accuracy: 0.9000 - auc: 0.9500 - val_loss: 2.5397 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 3.0067 - accuracy: 0.9000 - auc: 0.8900Passed epoch 7\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 3.0067 - accuracy: 0.9000 - auc: 0.8900 - val_loss: 2.5223 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5137 - accuracy: 0.9000 - auc: 0.9900Passed epoch 8\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 2.5137 - accuracy: 0.9000 - auc: 0.9900 - val_loss: 2.5364 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5607 - accuracy: 0.9000 - auc: 0.9900Passed epoch 9\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 2.5607 - accuracy: 0.9000 - auc: 0.9900 - val_loss: 2.5230 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.4219 - accuracy: 1.0000 - auc: 1.0000Passed epoch 10\n",
            "1/1 [==============================] - 1s 668ms/step - loss: 2.4219 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5283 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.4099 - accuracy: 1.0000 - auc: 1.0000Passed epoch 11\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 2.4099 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5341 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.4039 - accuracy: 1.0000 - auc: 1.0000Passed epoch 12\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 2.4039 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5413 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3989 - accuracy: 1.0000 - auc: 1.0000Passed epoch 13\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.3989 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5500 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3947 - accuracy: 1.0000 - auc: 1.0000Passed epoch 14\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 2.3947 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5602 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3910 - accuracy: 1.0000 - auc: 1.0000Passed epoch 15\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 2.3910 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5722 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3877 - accuracy: 1.0000 - auc: 1.0000Passed epoch 16\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 2.3877 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5860 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3847 - accuracy: 1.0000 - auc: 1.0000Passed epoch 17\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 2.3847 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6017 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3819 - accuracy: 1.0000 - auc: 1.0000Passed epoch 18\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 2.3819 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6194 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3794 - accuracy: 1.0000 - auc: 1.0000Passed epoch 19\n",
            "1/1 [==============================] - 1s 826ms/step - loss: 2.3794 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6391 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3770 - accuracy: 1.0000 - auc: 1.0000Passed epoch 20\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 2.3770 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6607 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3746 - accuracy: 1.0000 - auc: 1.0000Passed epoch 21\n",
            "1/1 [==============================] - 1s 679ms/step - loss: 2.3746 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6841 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3724 - accuracy: 1.0000 - auc: 1.0000Passed epoch 22\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 2.3724 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7088 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3701 - accuracy: 1.0000 - auc: 1.0000Passed epoch 23\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 2.3701 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7346 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3679 - accuracy: 1.0000 - auc: 1.0000Passed epoch 24\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 2.3679 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7609 - val_accuracy: 1.0000 - val_auc: 1.0000\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3657 - accuracy: 1.0000 - auc: 1.0000Passed epoch 25\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.3657 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7876 - val_accuracy: 0.7500 - val_auc: 0.9375\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3634 - accuracy: 1.0000 - auc: 1.0000Passed epoch 26\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 2.3634 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8144 - val_accuracy: 0.7500 - val_auc: 0.8750\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3611 - accuracy: 1.0000 - auc: 1.0000Passed epoch 27\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 2.3611 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8412 - val_accuracy: 0.7500 - val_auc: 0.8125\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3588 - accuracy: 1.0000 - auc: 1.0000Passed epoch 28\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 2.3588 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8678 - val_accuracy: 0.5000 - val_auc: 0.7500\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3563 - accuracy: 1.0000 - auc: 1.0000Passed epoch 29\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.3563 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8943 - val_accuracy: 0.5000 - val_auc: 0.7500\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3539 - accuracy: 1.0000 - auc: 1.0000Passed epoch 30\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.3539 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9206 - val_accuracy: 0.5000 - val_auc: 0.7500\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3513 - accuracy: 1.0000 - auc: 1.0000Passed epoch 31\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 2.3513 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9466 - val_accuracy: 0.5000 - val_auc: 0.6250\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3486 - accuracy: 1.0000 - auc: 1.0000Passed epoch 32\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.3486 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9724 - val_accuracy: 0.5000 - val_auc: 0.5625\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3459 - accuracy: 1.0000 - auc: 1.0000Passed epoch 33\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 2.3459 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9978 - val_accuracy: 0.5000 - val_auc: 0.5000\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3430 - accuracy: 1.0000 - auc: 1.0000Passed epoch 34\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 2.3430 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0231 - val_accuracy: 0.5000 - val_auc: 0.5000\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3400 - accuracy: 1.0000 - auc: 1.0000Passed epoch 35\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 2.3400 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0481 - val_accuracy: 0.5000 - val_auc: 0.5000\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3369 - accuracy: 1.0000 - auc: 1.0000Passed epoch 36\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.3369 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0728 - val_accuracy: 0.5000 - val_auc: 0.5000\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3337 - accuracy: 1.0000 - auc: 1.0000Passed epoch 37\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 2.3337 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0973 - val_accuracy: 0.2500 - val_auc: 0.4375\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3303 - accuracy: 1.0000 - auc: 1.0000Passed epoch 38\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 2.3303 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1215 - val_accuracy: 0.2500 - val_auc: 0.4375\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3268 - accuracy: 1.0000 - auc: 1.0000Passed epoch 39\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 2.3268 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1456 - val_accuracy: 0.2500 - val_auc: 0.4375\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3232 - accuracy: 1.0000 - auc: 1.0000Passed epoch 40\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.3232 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1694 - val_accuracy: 0.2500 - val_auc: 0.4375\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3194 - accuracy: 1.0000 - auc: 1.0000Passed epoch 41\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 2.3194 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1931 - val_accuracy: 0.2500 - val_auc: 0.3125\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3154 - accuracy: 1.0000 - auc: 1.0000Passed epoch 42\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.3154 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2166 - val_accuracy: 0.2500 - val_auc: 0.1875\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3113 - accuracy: 1.0000 - auc: 1.0000Passed epoch 43\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 2.3113 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2398 - val_accuracy: 0.2500 - val_auc: 0.1875\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3070 - accuracy: 1.0000 - auc: 1.0000Passed epoch 44\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 2.3070 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2627 - val_accuracy: 0.2500 - val_auc: 0.1875\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 1.0000 - auc: 1.0000Passed epoch 45\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 2.3026 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2853 - val_accuracy: 0.2500 - val_auc: 0.1875\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2980 - accuracy: 1.0000 - auc: 1.0000Passed epoch 46\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.2980 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3076 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2932 - accuracy: 1.0000 - auc: 1.0000Passed epoch 47\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 2.2932 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3296 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2882 - accuracy: 1.0000 - auc: 1.0000Passed epoch 48\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 2.2882 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3512 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2830 - accuracy: 1.0000 - auc: 1.0000Passed epoch 49\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 2.2830 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3723 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2776 - accuracy: 1.0000 - auc: 1.0000Passed epoch 50\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 2.2776 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3929 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2720 - accuracy: 1.0000 - auc: 1.0000Passed epoch 51\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 2.2720 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4130 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2661 - accuracy: 1.0000 - auc: 1.0000Passed epoch 52\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 2.2661 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4325 - val_accuracy: 0.2500 - val_auc: 0.0625\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2601 - accuracy: 1.0000 - auc: 1.0000Passed epoch 53\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 2.2601 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4513 - val_accuracy: 0.2500 - val_auc: 0.0312\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2538 - accuracy: 1.0000 - auc: 1.0000Passed epoch 54\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 2.2538 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4694 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2472 - accuracy: 1.0000 - auc: 1.0000Passed epoch 55\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 2.2472 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4867 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2405 - accuracy: 1.0000 - auc: 1.0000Passed epoch 56\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.2405 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5032 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2334 - accuracy: 1.0000 - auc: 1.0000Passed epoch 57\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.2334 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5189 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2261 - accuracy: 1.0000 - auc: 1.0000Passed epoch 58\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.2261 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5336 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2186 - accuracy: 1.0000 - auc: 1.0000Passed epoch 59\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 2.2186 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5474 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2107 - accuracy: 1.0000 - auc: 1.0000Passed epoch 60\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 2.2107 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5601 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2026 - accuracy: 1.0000 - auc: 1.0000Passed epoch 61\n",
            "1/1 [==============================] - 1s 669ms/step - loss: 2.2026 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5717 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1942 - accuracy: 1.0000 - auc: 1.0000Passed epoch 62\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 2.1942 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5822 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1854 - accuracy: 1.0000 - auc: 1.0000Passed epoch 63\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 2.1854 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5915 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1764 - accuracy: 1.0000 - auc: 1.0000Passed epoch 64\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 2.1764 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5996 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1671 - accuracy: 1.0000 - auc: 1.0000Passed epoch 65\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.1671 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6065 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1574 - accuracy: 1.0000 - auc: 1.0000Passed epoch 66\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 2.1574 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6120 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1474 - accuracy: 1.0000 - auc: 1.0000Passed epoch 67\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 2.1474 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6163 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1371 - accuracy: 1.0000 - auc: 1.0000Passed epoch 68\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 2.1371 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6191 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1265 - accuracy: 1.0000 - auc: 1.0000Passed epoch 69\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.1265 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6206 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1155 - accuracy: 1.0000 - auc: 1.0000Passed epoch 70\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 2.1155 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6206 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1041 - accuracy: 1.0000 - auc: 1.0000Passed epoch 71\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 2.1041 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6191 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0924 - accuracy: 1.0000 - auc: 1.0000Passed epoch 72\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 2.0924 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6162 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0803 - accuracy: 1.0000 - auc: 1.0000Passed epoch 73\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 2.0803 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6119 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0679 - accuracy: 1.0000 - auc: 1.0000Passed epoch 74\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 2.0679 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.6061 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0551 - accuracy: 1.0000 - auc: 1.0000Passed epoch 75\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.0551 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5989 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0419 - accuracy: 1.0000 - auc: 1.0000Passed epoch 76\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 2.0419 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5902 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0284 - accuracy: 1.0000 - auc: 1.0000Passed epoch 77\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 2.0284 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5800 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0144 - accuracy: 1.0000 - auc: 1.0000Passed epoch 78\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 2.0144 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5685 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0001 - accuracy: 1.0000 - auc: 1.0000Passed epoch 79\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 2.0001 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5556 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9854 - accuracy: 1.0000 - auc: 1.0000Passed epoch 80\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 1.9854 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5414 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9703 - accuracy: 1.0000 - auc: 1.0000Passed epoch 81\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 1.9703 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5259 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9549 - accuracy: 1.0000 - auc: 1.0000Passed epoch 82\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 1.9549 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.5092 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9390 - accuracy: 1.0000 - auc: 1.0000Passed epoch 83\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 1.9390 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4913 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9228 - accuracy: 1.0000 - auc: 1.0000Passed epoch 84\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 1.9228 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4722 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9062 - accuracy: 1.0000 - auc: 1.0000Passed epoch 85\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 1.9062 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4521 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8892 - accuracy: 1.0000 - auc: 1.0000Passed epoch 86\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 1.8892 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4309 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8718 - accuracy: 1.0000 - auc: 1.0000Passed epoch 87\n",
            "1/1 [==============================] - 1s 660ms/step - loss: 1.8718 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.4088 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8540 - accuracy: 1.0000 - auc: 1.0000Passed epoch 88\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 1.8540 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3857 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8359 - accuracy: 1.0000 - auc: 1.0000Passed epoch 89\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 1.8359 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3619 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8174 - accuracy: 1.0000 - auc: 1.0000Passed epoch 90\n",
            "1/1 [==============================] - 1s 661ms/step - loss: 1.8174 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3371 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7986 - accuracy: 1.0000 - auc: 1.0000Passed epoch 91\n",
            "1/1 [==============================] - 1s 657ms/step - loss: 1.7986 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.3117 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7794 - accuracy: 1.0000 - auc: 1.0000Passed epoch 92\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 1.7794 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2854 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7599 - accuracy: 1.0000 - auc: 1.0000Passed epoch 93\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 1.7599 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2586 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7400 - accuracy: 1.0000 - auc: 1.0000Passed epoch 94\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 1.7400 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2312 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7199 - accuracy: 1.0000 - auc: 1.0000Passed epoch 95\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 1.7199 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.2033 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6994 - accuracy: 1.0000 - auc: 1.0000Passed epoch 96\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 1.6994 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1748 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6786 - accuracy: 1.0000 - auc: 1.0000Passed epoch 97\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 1.6786 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1458 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6575 - accuracy: 1.0000 - auc: 1.0000Passed epoch 98\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 1.6575 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.1165 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6361 - accuracy: 1.0000 - auc: 1.0000Passed epoch 99\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 1.6361 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0868 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6145 - accuracy: 1.0000 - auc: 1.0000Passed epoch 100\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 1.6145 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0567 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5927 - accuracy: 1.0000 - auc: 1.0000Passed epoch 101\n",
            "1/1 [==============================] - 1s 656ms/step - loss: 1.5927 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 3.0264 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5705 - accuracy: 1.0000 - auc: 1.0000Passed epoch 102\n",
            "1/1 [==============================] - 1s 655ms/step - loss: 1.5705 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9958 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5482 - accuracy: 1.0000 - auc: 1.0000Passed epoch 103\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 1.5482 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9649 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5257 - accuracy: 1.0000 - auc: 1.0000Passed epoch 104\n",
            "1/1 [==============================] - 1s 674ms/step - loss: 1.5257 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9338 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5029 - accuracy: 1.0000 - auc: 1.0000Passed epoch 105\n",
            "1/1 [==============================] - 1s 659ms/step - loss: 1.5029 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.9027 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4800 - accuracy: 1.0000 - auc: 1.0000Passed epoch 106\n",
            "1/1 [==============================] - 1s 662ms/step - loss: 1.4800 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8713 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4570 - accuracy: 1.0000 - auc: 1.0000Passed epoch 107\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 1.4570 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8399 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4338 - accuracy: 1.0000 - auc: 1.0000Passed epoch 108\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 1.4338 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.8084 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4105 - accuracy: 1.0000 - auc: 1.0000Passed epoch 109\n",
            "1/1 [==============================] - 1s 665ms/step - loss: 1.4105 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7768 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3871 - accuracy: 1.0000 - auc: 1.0000Passed epoch 110\n",
            "1/1 [==============================] - 1s 676ms/step - loss: 1.3871 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7453 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3636 - accuracy: 1.0000 - auc: 1.0000Passed epoch 111\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 1.3636 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.7137 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3401 - accuracy: 1.0000 - auc: 1.0000Passed epoch 112\n",
            "1/1 [==============================] - 1s 667ms/step - loss: 1.3401 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6821 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3165 - accuracy: 1.0000 - auc: 1.0000Passed epoch 113\n",
            "1/1 [==============================] - 1s 651ms/step - loss: 1.3165 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6510 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2929 - accuracy: 1.0000 - auc: 1.0000Passed epoch 114\n",
            "1/1 [==============================] - 1s 658ms/step - loss: 1.2929 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.6193 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2693 - accuracy: 1.0000 - auc: 1.0000Passed epoch 115\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 1.2693 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5887 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2457 - accuracy: 1.0000 - auc: 1.0000Passed epoch 116\n",
            "1/1 [==============================] - 1s 672ms/step - loss: 1.2457 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5571 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2221 - accuracy: 1.0000 - auc: 1.0000Passed epoch 117\n",
            "1/1 [==============================] - 1s 653ms/step - loss: 1.2221 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.5271 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1986 - accuracy: 1.0000 - auc: 1.0000Passed epoch 118\n",
            "1/1 [==============================] - 1s 639ms/step - loss: 1.1986 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.4958 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1751 - accuracy: 1.0000 - auc: 1.0000Passed epoch 119\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 1.1751 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.4660 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1517 - accuracy: 1.0000 - auc: 1.0000Passed epoch 120\n",
            "1/1 [==============================] - 1s 673ms/step - loss: 1.1517 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.4353 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1284 - accuracy: 1.0000 - auc: 1.0000Passed epoch 121\n",
            "1/1 [==============================] - 1s 649ms/step - loss: 1.1284 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.4057 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1053 - accuracy: 1.0000 - auc: 1.0000Passed epoch 122\n",
            "1/1 [==============================] - 1s 663ms/step - loss: 1.1053 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.3756 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0822 - accuracy: 1.0000 - auc: 1.0000Passed epoch 123\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 1.0822 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.3465 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0594 - accuracy: 1.0000 - auc: 1.0000Passed epoch 124\n",
            "1/1 [==============================] - 1s 644ms/step - loss: 1.0594 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.3164 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0367 - accuracy: 1.0000 - auc: 1.0000Passed epoch 125\n",
            "1/1 [==============================] - 1s 670ms/step - loss: 1.0367 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.2891 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0142 - accuracy: 1.0000 - auc: 1.0000Passed epoch 126\n",
            "1/1 [==============================] - 1s 664ms/step - loss: 1.0142 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.2587 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9918 - accuracy: 1.0000 - auc: 1.0000Passed epoch 127\n",
            "1/1 [==============================] - 1s 654ms/step - loss: 0.9918 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 2.2358 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00\n",
            "Epoch 129/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNVx_Dbrr-_u",
        "outputId": "5b7f4a05-f0ca-4e58-ae36-fb52c2d6cbcc"
      },
      "source": [
        "\n",
        "print('Metric '+pers_act)\n",
        "\n",
        "print(report_average(aa))\n",
        "print('AUC')\n",
        "print(np.mean(aucs))\n",
        "\n",
        "print('Metric Sinusoidal')\n",
        "\n",
        "print(report_average(saa))\n",
        "print('AUC')\n",
        "print(np.mean(saucs))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric elu\n",
            "{'0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2}}\n",
            "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, '1': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 2}}\n",
            "{'0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
            "{'0': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, '1': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'accuracy': 0.5, 'macro avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'weighted avg': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}}\n",
            "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, '1': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
            "{'0': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'accuracy': 0.5, 'macro avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}, 'weighted avg': {'precision': 0.25, 'recall': 0.5, 'f1-score': 0.3333333333333333, 'support': 4}}\n",
            "Mean accuracy 0.5\n",
            "Mean precision 0.2916666666666667\n",
            "Mean recall 0.5\n",
            "Mean F1 0.3611111111111111\n",
            "None\n",
            "AUC\n",
            "0.6458333333333334\n",
            "Metric Sinusoidal\n",
            "Mean accuracy nan\n",
            "Mean precision nan\n",
            "Mean recall nan\n",
            "Mean F1 nan\n",
            "None\n",
            "AUC\n",
            "nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    }
  ]
}